{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151fd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax import struct\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Any\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cafc054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fda83cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d012c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for model parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = tokenizer.vocab_size\n",
    "    max_seq_len: int = 1024\n",
    "    d_model: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    d_ff: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    lr: float = 6e-4\n",
    "    warmup_steps: int = 700\n",
    "    total_steps: int = 20000\n",
    "    batch_size: int = 64\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    mixed_precision: bool = False\n",
    "    num_epochs: int = 1\n",
    "    \n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16494b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "        self.d_Q = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_K = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_V = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_O = nn.Dense(features=self.d_model, use_bias=False)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "            B,T,C = x.shape\n",
    "            query = self.d_Q(x)\n",
    "            key = self.d_K(x)\n",
    "            value = self.d_V(x)\n",
    "            \n",
    "            weights = jnp.matmul(query, key.transpose(0,2, 1)) * (key.shape[-1] ** -0.5)\n",
    "            mask = jnp.tril(jnp.ones((T,T)))\n",
    "            mask = jnp.where(mask==0, -1e9, 1.0)\n",
    "            weights = weights * mask\n",
    "            weights = nn.softmax(weights, axis=-1)\n",
    "            out = jnp.matmul(weights, value)\n",
    "            out = self.d_O(out)\n",
    "            out = self.dropout(out, deterministic=not training)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c23bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jnp.tril(jnp.ones((128,128)))\n",
    "mask = jnp.where(mask==0, -1e9, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa5c50c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.e+00, -1.e+09, -1.e+09, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00, -1.e+09, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       ...,\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00,  1.e+00, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00,  1.e+00,  1.e+00]],      dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6eb96ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.heads = [Attention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "        self.linear = nn.Dense(features=self.d_model)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60072f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attention():\n",
    "    attn = MHA()\n",
    "    x = jnp.ones((2, 128, config.d_model))\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = attn.init(rng, x, training=True)\n",
    "    out = attn.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == x.shape, f\"Expected {x.shape}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af9d5276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1fd804b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.fc1 = nn.Dense(features=self.d_ff)\n",
    "        self.fc2 = nn.Dense(features=self.d_model)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eeb03796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_mlp():\n",
    "    mlp = MLP()\n",
    "    x = jnp.ones((2, 128, config.d_model))\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = mlp.init(rng, x, training=True)\n",
    "    out = mlp.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == x.shape, f\"Expected {x.shape}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8ccb08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c1b2a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.attention = MHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm()\n",
    "        self.ln2 = nn.LayerNorm()\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        x = x + self.attention(self.ln1(x), training)\n",
    "        x = x + self.mlp(self.ln2(x), training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c83412e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    vocab_size: int = config.vocab_size\n",
    "    seq_len: int = config.max_seq_len\n",
    "    \n",
    "    def setup(self):\n",
    "        self.embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model)\n",
    "        self.positional_embedding = self.param(\n",
    "            \"positional_embeddings\",  # name\n",
    "            lambda key: jax.random.normal(key, (1, self.seq_len, self.d_model)) * 0.02\n",
    "        )\n",
    "        self.decoder = [TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(config.num_layers)]\n",
    "        self.linear_out = nn.Dense(features=self.vocab_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        B,T = x.shape\n",
    "        embeds = self.embedding_table(x)  # (B,T,d_model)\n",
    "        C = embeds.shape[-1]\n",
    "        pos_embeds = self.positional_embedding[:, :T, :]  # (1,T,d_model)\n",
    "        x = embeds + pos_embeds  # (B,T,d_model)\n",
    "        pad_mask = (x != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "        x = x * pad_mask\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, training=training)\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20290ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt():\n",
    "    model = GPT()\n",
    "    x = jnp.ones((2, 128), dtype=jnp.int32)\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = model.init(rng, x, training=True)\n",
    "    out = model.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1), \"positional_embeddings\": jax.random.PRNGKey(2)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == (2, 128, config.vocab_size), f\"Expected {(2, 128, config.vocab_size)}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "047005f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 50257)\n"
     ]
    }
   ],
   "source": [
    "test_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "57b39759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_rate_schedule():\n",
    "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
    "    config = GPTConfig()\n",
    "    def schedule(step):\n",
    "        # Linear warmup\n",
    "        warmup_ratio = jnp.minimum(1.0, step / config.warmup_steps)\n",
    "        # Cosine decay after warmup\n",
    "        decay_ratio = jnp.maximum(0.0, (step - config.warmup_steps) / (config.total_steps - config.warmup_steps))\n",
    "        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * decay_ratio))\n",
    "        return config.lr * warmup_ratio * cosine_decay\n",
    "\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "687a1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    labels = labels[:, 1:]\n",
    "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
    "    \n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "23fd1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "    model = GPT()\n",
    "    \n",
    "    # Initialize parameters\n",
    "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
    "    params = model.init(rng, dummy_input)['params']\n",
    "    \n",
    "    # Create learning rate schedule\n",
    "    lr_schedule = create_learning_rate_schedule()\n",
    "    \n",
    "    # Create optimizer\n",
    "    tx = optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.95,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "393e62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(jax.random.PRNGKey(0), config)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "79af46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True)\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "    \n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    \n",
    "    #update the parametrs\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compiled evaluation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    \"\"\"Single evaluation step.\"\"\"\n",
    "    logits = state.apply_fn({'params': state.params}, batch, training=False)\n",
    "    loss = compute_ce_loss(logits, batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12a9d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compiled prediction step\n",
    "@jax.jit\n",
    "def predict_batch(state, batch):\n",
    "    \"\"\"Generate predictions for a batch using vmap.\"\"\"\n",
    "    return state.apply_fn({'params': state.params}, batch, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    train_state = create_train_state(rng, config)\n",
    "    # Training loop\n",
    "    num_epochs = config.num_epochs\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training\n",
    "        state = train_state.replace(step=0)  # Reset step counter for LR schedule\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        # Process in batches with progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in pbar:\n",
    "            # Convert to JAX array\n",
    "            batch = jnp.array(batch)\n",
    "            \n",
    "            state, loss = train_step(state, batch)\n",
    "            train_losses.append(loss)\n",
    "            train_accs.append(acc)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss:.4f}\", \n",
    "                \"acc\": f\"{acc:.4f}\",\n",
    "                \"lr\": f\"{state.opt_state[1].hyperparams['learning_rate']:.6f}\"\n",
    "            })\n",
    "        \n",
    "        # Validation\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        \n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        for batch in pbar:\n",
    "            batch = jnp.array(batch)\n",
    "            loss, acc = eval_step(state, batch, tokenizer.pad_token_id)\n",
    "            val_losses.append(loss)\n",
    "            val_accs.append(acc)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss:.4f}\", \n",
    "                \"acc\": f\"{acc:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_train_acc = np.mean(train_accs)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        avg_val_acc = np.mean(val_accs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save model checkpoint\n",
    "            print(f\"  New best model! Saving checkpoint...\")\n",
    "    \n",
    "    # Generate some text\n",
    "    print(\"\\nGenerating text...\")\n",
    "    generated = generate_text(state, \"The future of artificial intelligence\", tokenizer, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
