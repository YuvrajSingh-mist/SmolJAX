{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"jax[cuda12]\"\n",
        "!pip install wandb\n",
        "!pip install tqdm\n",
        "!pip install optax\n",
        "!pip install flax\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ICLM60p0rP9",
        "outputId": "6baa49cd-2cae-415a-98e9-876d576b3ad8"
      },
      "id": "5ICLM60p0rP9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.7.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (1.16.2)\n",
            "Requirement already satisfied: jax-cuda12-plugin<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (0.7.2)\n",
            "Requirement already satisfied: jax-cuda12-pjrt==0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin<=0.7.2,>=0.7.2->jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (0.7.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.6.85 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.9.86)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.8 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12>=12.1.55 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12>=3.2.5 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with-cuda]<=0.7.2,>=0.7.2; extra == \"cuda12\"->jax[cuda12]) (3.4.5)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.38.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jax>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.7.2)\n",
            "Requirement already satisfied: jaxlib>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax) (2.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (1.16.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.10.6)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax) (2.0.2)\n",
            "Requirement already satisfied: jax>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from flax) (0.7.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax) (0.11.24)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (0.7.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (1.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.23.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3D8_qec1GTW",
        "outputId": "2a05dc65-31d7-42b0-be53-599327fc9bc7"
      },
      "id": "N3D8_qec1GTW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "151fd62f",
      "metadata": {
        "id": "151fd62f"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.training import checkpoints\n",
        "from flax import struct\n",
        "import optax\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Any\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "502276f0",
      "metadata": {
        "id": "502276f0"
      },
      "outputs": [],
      "source": [
        "from flax.training import train_state\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    grad_accum: Any = None\n",
        "    accum_step: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "1f8e0fc675fa4ab7b55078d65bf0b3ce",
            "977918c696b44e2b850b99e943f8337a",
            "6cfee82ae404423ba0bf4edead5a446e",
            "299804612e1d42ff8576d65336b87489",
            "7edda5d229d24813ab76d26681756ea5",
            "88094f7cd20f4ae6aeab0655fd6e0a16",
            "392cef72dc0b48d29b46d555570ebf7a",
            "7fef56ad4eed4071af3cec0c37d645cb",
            "2d39b21fefca42c09f297c0698f1c218",
            "0db380497d9e4996978568642cc65cb1",
            "69c861a9812c48709f5476dc7acba4c8",
            "c9efd464006840148dd16349f2e1f565",
            "1dbd561c6d524d96aa63645835d7b120",
            "edecfa1aa55d49248b16a48d575e5608",
            "e71900a34d6446a39c2dbc27f3d6e751",
            "b52f4b93f2224761966ccff094b1b77d",
            "53df73a036174709aaf37eb119475df1"
          ]
        },
        "id": "5iJl8gve4ZRI",
        "outputId": "064ecaea-0498-4a78-c5b0-751d084f3bf6"
      },
      "id": "5iJl8gve4ZRI",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f8e0fc675fa4ab7b55078d65bf0b3ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cafc054c",
      "metadata": {
        "id": "cafc054c"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=\"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\")\n",
        "tokenizer.pad_token = '[PAD]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2fa95aed",
      "metadata": {
        "id": "2fa95aed"
      },
      "outputs": [],
      "source": [
        "#Loading TinyStories dataset from Huggingface\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", token='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')\n",
        "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d012c592",
      "metadata": {
        "id": "d012c592"
      },
      "outputs": [],
      "source": [
        "# Configuration class for model parameters\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int = tokenizer.vocab_size\n",
        "    max_seq_len: int = 128\n",
        "    d_model: int = 768\n",
        "    num_layers: int = 8\n",
        "    num_heads: int = 8\n",
        "    d_ff: int = 3072\n",
        "    dropout_rate: float = 0.1\n",
        "    lr: float = 6e-4\n",
        "    warmup_steps: int = 700\n",
        "    total_steps: int = 20000\n",
        "    batch_size: int = 64\n",
        "    required_bsz: int = 524288\n",
        "    gradient_accumulation_steps: int = int(required_bsz // (batch_size * max_seq_len))\n",
        "    mixed_precision: bool = False\n",
        "    num_epochs: int = 1\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "16494b05",
      "metadata": {
        "id": "16494b05"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.head_size = self.d_model // self.num_heads\n",
        "        self.d_Q = nn.Dense(features=self.head_size, use_bias=False)\n",
        "        self.d_K = nn.Dense(features=self.head_size, use_bias=False)\n",
        "        self.d_V = nn.Dense(features=self.head_size, use_bias=False)\n",
        "        self.d_O = nn.Dense(features=self.d_model, use_bias=False)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "            B,T,C = x.shape\n",
        "            query = self.d_Q(x)\n",
        "            key = self.d_K(x)\n",
        "            value = self.d_V(x)\n",
        "\n",
        "            weights = jnp.matmul(query, key.transpose(0,2, 1)) * (key.shape[-1] ** -0.5)\n",
        "            mask = jnp.tril(jnp.ones((T,T)))\n",
        "            mask = jnp.where(mask==0, -1e9, 1.0)\n",
        "            weights = weights * mask\n",
        "            weights = nn.softmax(weights, axis=-1)\n",
        "            out = jnp.matmul(weights, value)\n",
        "            out = self.d_O(out)\n",
        "            out = self.dropout(out, deterministic=not training)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6eb96ba1",
      "metadata": {
        "id": "6eb96ba1"
      },
      "outputs": [],
      "source": [
        "class MHA(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.heads = [Attention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
        "        self.linear = nn.Dense(features=self.d_model)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out, deterministic=not training)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1fd804b6",
      "metadata": {
        "id": "1fd804b6"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.fc1 = nn.Dense(features=self.d_ff)\n",
        "        self.fc2 = nn.Dense(features=self.d_model)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        x = self.fc1(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c1b2a846",
      "metadata": {
        "id": "c1b2a846"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.attention = MHA(self.d_model, self.num_heads, self.dropout_rate)\n",
        "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
        "        self.ln1 = nn.LayerNorm()\n",
        "        self.ln2 = nn.LayerNorm()\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        x = x + self.attention(self.ln1(x), training)\n",
        "        x = x + self.mlp(self.ln2(x), training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c83412e4",
      "metadata": {
        "id": "c83412e4"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "    vocab_size: int = config.vocab_size\n",
        "    seq_len: int = config.max_seq_len\n",
        "\n",
        "    def setup(self):\n",
        "        self.embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model)\n",
        "        self.positional_embedding = self.param(\n",
        "            \"positional_embeddings\",  # name\n",
        "            lambda key: jax.random.normal(key, (1, self.seq_len, self.d_model)) * 0.02\n",
        "        )\n",
        "        self.decoder = [TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(config.num_layers)]\n",
        "        self.linear_out = nn.Dense(features=self.vocab_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        B,T = x.shape\n",
        "        embeds = self.embedding_table(x)  # (B,T,d_model)\n",
        "        C = embeds.shape[-1]\n",
        "        pos_embeds = self.positional_embedding[:, :T, :]  # (1,T,d_model)\n",
        "        x = embeds + pos_embeds  # (B,T,d_model)\n",
        "        pad_mask = (x != tokenizer.pad_token_id).astype(jnp.float32)\n",
        "        x = x * pad_mask\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x, training=training)\n",
        "\n",
        "        x = self.linear_out(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b6eb78ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6eb78ff",
        "outputId": "7084eee7-06d9-4183-8072-ebc2105aae52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary saved to model_summary.txt\n",
            "Total Parameters: 171,776,593\n",
            "Model size: ~655.3 MB (float32)\n"
          ]
        }
      ],
      "source": [
        "# Add this cell to inspect the model summary like torchsummary\n",
        "\n",
        "from flax.linen import tabulate\n",
        "import jax\n",
        "\n",
        "# Initialize model\n",
        "model = GPT()\n",
        "key = jax.random.PRNGKey(0)\n",
        "x = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # Dummy input for tabulation\n",
        "\n",
        "# Tabulate the model structure\n",
        "tabulate_fn = tabulate(model, key, console_kwargs={'width': 120})\n",
        "\n",
        "# Count total parameters\n",
        "params = model.init(key, x)['params']\n",
        "total_params = sum(jax.tree_util.tree_leaves(jax.tree.map(lambda arr: arr.size, params)))\n",
        "\n",
        "# Get raw summary and clean ANSI codes\n",
        "raw_summary = tabulate_fn(x, training=True)\n",
        "# Remove ANSI color codes for clean logging\n",
        "clean_summary = re.sub(r'\\x1b\\[[0-9;]*m', '', raw_summary)\n",
        "\n",
        "# Save to log file with clean formatting\n",
        "with open('model_summary.txt', 'w') as f:\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(\"GPT MODEL ARCHITECTURE SUMMARY\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
        "    f.write(f\"Model Configuration:\\n\")\n",
        "    f.write(f\"  - Vocabulary Size: {config.vocab_size:,}\\n\")\n",
        "    f.write(f\"  - Max Sequence Length: {config.max_seq_len}\\n\")\n",
        "    f.write(f\"  - Model Dimension: {config.d_model}\\n\")\n",
        "    f.write(f\"  - Number of Layers: {config.num_layers}\\n\")\n",
        "    f.write(f\"  - Number of Heads: {config.num_heads}\\n\")\n",
        "    f.write(f\"  - Feed Forward Dimension: {config.d_ff}\\n\")\n",
        "    f.write(f\"  - Dropout Rate: {config.dropout_rate}\\n\\n\")\n",
        "    f.write(\"Detailed Layer Information:\\n\")\n",
        "    f.write(\"-\" * 40 + \"\\n\")\n",
        "    f.write(clean_summary)\n",
        "\n",
        "print(f\"Model summary saved to model_summary.txt\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / (1024**2):.1f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "57b39759",
      "metadata": {
        "id": "57b39759"
      },
      "outputs": [],
      "source": [
        "def create_learning_rate_schedule():\n",
        "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
        "    config = GPTConfig()\n",
        "    def schedule(step):\n",
        "        # Linear warmup\n",
        "        warmup_ratio = jnp.minimum(1.0, step / config.warmup_steps)\n",
        "        # Cosine decay after warmup\n",
        "        decay_ratio = jnp.maximum(0.0, (step - config.warmup_steps) / (config.total_steps - config.warmup_steps))\n",
        "        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * decay_ratio))\n",
        "        return config.lr * warmup_ratio * cosine_decay\n",
        "\n",
        "    return schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "687a1842",
      "metadata": {
        "id": "687a1842"
      },
      "outputs": [],
      "source": [
        "def compute_ce_loss(logits, labels):\n",
        "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
        "    labels = labels[:, 1:]\n",
        "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
        "\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "23fd1398",
      "metadata": {
        "id": "23fd1398"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, config):\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "    model = GPT()\n",
        "\n",
        "    # Initialize parameters\n",
        "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
        "    params = model.init(rng, dummy_input)['params']\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    lr_schedule = create_learning_rate_schedule()\n",
        "\n",
        "    # Create optimizer\n",
        "    tx = optax.adamw(\n",
        "        learning_rate=lr_schedule,\n",
        "        b1=0.9,\n",
        "        b2=0.95,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=tx\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "79af46bc",
      "metadata": {
        "id": "79af46bc"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({\"params\": params}, batch, training=True)\n",
        "        loss = compute_ce_loss(logits, batch)\n",
        "        return loss, logits\n",
        "\n",
        "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "\n",
        "    # Compute gradient norm for logging\n",
        "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
        "\n",
        "    # Update the parameters\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss, grad_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35c7ffe7",
      "metadata": {
        "id": "35c7ffe7"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step_accum(state, batch, step):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
        "        loss = compute_ce_loss(logits, batch)\n",
        "        return loss, logits\n",
        "\n",
        "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "    # Compute gradient norm for logging\n",
        "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
        "\n",
        "    if state.grad_accum is None:\n",
        "\n",
        "        state = state.replace(\n",
        "            grad_accum=jax.tree_util.tree_map(jnp.zeros_like, grads),\n",
        "            accum_step=0,\n",
        "        )\n",
        "    new_accum = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, state.grad_accum, grads)\n",
        "    new_step = state.accum_step + 1\n",
        "\n",
        "    # print(new_accum)\n",
        "    # print(new_step)\n",
        "\n",
        "    def apply_update(_):\n",
        "        mean_grads = jax.tree_util.tree_map(lambda g: g / config.gradient_accumulation_steps, new_accum)\n",
        "        new_state = state.apply_gradients(grads = mean_grads)\n",
        "        new_grad_accum = jax.tree_util.tree_map(jnp.zeros_like, new_accum)\n",
        "        return new_state.replace(grad_accum=new_grad_accum, accum_step=0)\n",
        "\n",
        "    def carry_forward(_):\n",
        "\n",
        "        # dummy_grads = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "\n",
        "        return state.replace(grad_accum=new_accum, accum_step=new_step)\n",
        "\n",
        "\n",
        "    state = jax.lax.cond(new_step == config.gradient_accumulation_steps, apply_update, carry_forward, operand=None)\n",
        "\n",
        "    return state, loss, grad_norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f95fc0cc",
      "metadata": {
        "id": "f95fc0cc"
      },
      "outputs": [],
      "source": [
        "# JIT-compiled evaluation step\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    \"\"\"Single evaluation step.\"\"\"\n",
        "    logits = state.apply_fn({'params': state.params}, batch, training=False)\n",
        "    loss = compute_ce_loss(logits, batch)\n",
        "\n",
        "    return loss, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "12a9d0a7",
      "metadata": {
        "id": "12a9d0a7"
      },
      "outputs": [],
      "source": [
        "# Vectorized prediction function using vmap\n",
        "@jax.jit\n",
        "def predict_batch(state, batch):\n",
        "    \"\"\"Generate predictions for a batch using vmap.\"\"\"\n",
        "    return state.apply_fn({'params': state.params}, batch, training=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "63c34940",
      "metadata": {
        "id": "63c34940"
      },
      "outputs": [],
      "source": [
        "# Helper function to create wandb summary table\n",
        "def log_training_summary(state, config, total_params, tokens_processed):\n",
        "    \"\"\"Log a comprehensive training summary to wandb.\"\"\"\n",
        "\n",
        "    # Create a summary table (ensure all values are strings for wandb compatibility)\n",
        "    summary_data = [\n",
        "        [\"Model\", \"SmolJAX GPT\"],\n",
        "        [\"Total Parameters\", f\"{total_params:,}\"],\n",
        "        [\"Model Size (MB)\", f\"{total_params * 4 / (1024**2):.1f}\"],\n",
        "        [\"Vocabulary Size\", f\"{config.vocab_size:,}\"],\n",
        "        [\"Max Sequence Length\", f\"{config.max_seq_len}\"],\n",
        "        [\"Model Dimension\", f\"{config.d_model}\"],\n",
        "        [\"Number of Layers\", f\"{config.num_layers}\"],\n",
        "        [\"Number of Heads\", f\"{config.num_heads}\"],\n",
        "        [\"Feed Forward Dimension\", f\"{config.d_ff}\"],\n",
        "        [\"Dropout Rate\", f\"{config.dropout_rate}\"],\n",
        "        [\"Learning Rate\", f\"{config.lr}\"],\n",
        "        [\"Batch Size\", f\"{config.batch_size}\"],\n",
        "        [\"Total Epochs\", f\"{config.num_epochs}\"],\n",
        "        [\"Tokens Processed\", f\"{tokens_processed:,}\"],\n",
        "        [\"Training Step\", f\"{int(state.step)}\"]\n",
        "    ]\n",
        "\n",
        "    # Create wandb table\n",
        "    table = wandb.Table(\n",
        "        columns=[\"Metric\", \"Value\"],\n",
        "        data=summary_data\n",
        "    )\n",
        "\n",
        "    wandb.log({\"training_summary\": table})\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e4a98639",
      "metadata": {
        "id": "e4a98639"
      },
      "outputs": [],
      "source": [
        "# Checkpoint management functions\n",
        "def save_checkpoint(state, step, checkpoint_dir=\"./checkpoints\", keep=5):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    checkpoints.save_checkpoint(\n",
        "        ckpt_dir=checkpoint_dir,\n",
        "        target=state,\n",
        "        step=step,\n",
        "        keep=keep,  # Keep only the last 5 checkpoints\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "    print(f\"Checkpoint saved at step {step} in {checkpoint_dir}\")\n",
        "\n",
        "    # Log to wandb if available\n",
        "    if wandb.run is not None:\n",
        "        wandb.log({\n",
        "            \"checkpoint/step\": step,\n",
        "            \"checkpoint/saved\": 1\n",
        "        }, step=step)\n",
        "\n",
        "def load_checkpoint(checkpoint_dir=\"./checkpoints\", state=None):\n",
        "    \"\"\"Load the latest checkpoint.\"\"\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
        "        return None, 0\n",
        "\n",
        "    # Check if there are any checkpoints\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
        "    if not checkpoint_files:\n",
        "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
        "        return None, 0\n",
        "\n",
        "    try:\n",
        "        # Load the latest checkpoint\n",
        "        restored_state = checkpoints.restore_checkpoint(\n",
        "            ckpt_dir=checkpoint_dir,\n",
        "            target=state\n",
        "        )\n",
        "\n",
        "        # Get the step number from the checkpoint\n",
        "        latest_step = checkpoints.latest_checkpoint(checkpoint_dir)\n",
        "        if latest_step:\n",
        "            step = int(latest_step.split('_')[-1])\n",
        "            print(f\"Checkpoint loaded from step {step}\")\n",
        "            return restored_state, step\n",
        "        else:\n",
        "            print(f\"Could not determine step from checkpoint\")\n",
        "            return None, 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "def get_checkpoint_info(checkpoint_dir=\"./checkpoints\"):\n",
        "    \"\"\"Get information about available checkpoints.\"\"\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        return []\n",
        "\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
        "    checkpoint_steps = []\n",
        "\n",
        "    for file in checkpoint_files:\n",
        "        try:\n",
        "            step = int(file.split('_')[-1])\n",
        "            checkpoint_steps.append(step)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return sorted(checkpoint_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "522e41b1",
      "metadata": {
        "id": "522e41b1"
      },
      "outputs": [],
      "source": [
        "# Model-only saving/loading functions for inference\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "def save_model_for_inference(state, model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Save only model parameters and config for inference (much smaller files).\"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save just the parameters (no optimizer state)\n",
        "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
        "    with open(params_path, 'wb') as f:\n",
        "        pickle.dump(state.params, f)\n",
        "\n",
        "    # Save model configuration\n",
        "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
        "    config_dict = {\n",
        "        \"vocab_size\": config.vocab_size,\n",
        "        \"max_seq_len\": config.max_seq_len,\n",
        "        \"d_model\": config.d_model,\n",
        "        \"num_layers\": config.num_layers,\n",
        "        \"num_heads\": config.num_heads,\n",
        "        \"d_ff\": config.d_ff,\n",
        "        \"dropout_rate\": config.dropout_rate\n",
        "    }\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "\n",
        "    # Save tokenizer info\n",
        "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
        "    tokenizer_info = {\n",
        "        \"tokenizer_name\": \"gpt2\",\n",
        "        \"vocab_size\": len(tokenizer),\n",
        "        \"pad_token\": tokenizer.pad_token,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token\": tokenizer.eos_token,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "    with open(tokenizer_path, 'w') as f:\n",
        "        json.dump(tokenizer_info, f, indent=2)\n",
        "\n",
        "    print(f\"   Model saved for inference:\")\n",
        "    print(f\"   Parameters: {params_path}\")\n",
        "    print(f\"   Config: {config_path}\")\n",
        "    print(f\"   Tokenizer info: {tokenizer_path}\")\n",
        "\n",
        "    # Calculate file sizes\n",
        "    params_size = os.path.getsize(params_path) / (1024**2)  # MB\n",
        "    print(f\"   Model size: {params_size:.1f} MB\")\n",
        "\n",
        "    return params_path, config_path, tokenizer_path\n",
        "\n",
        "def load_model_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Load model parameters and config for inference.\"\"\"\n",
        "\n",
        "    # Load configuration\n",
        "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "\n",
        "    # Recreate config object\n",
        "    inference_config = GPTConfig(**config_dict)\n",
        "\n",
        "    # Load parameters\n",
        "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
        "    if not os.path.exists(params_path):\n",
        "        raise FileNotFoundError(f\"Parameters file not found: {params_path}\")\n",
        "\n",
        "    with open(params_path, 'rb') as f:\n",
        "        params = pickle.load(f)\n",
        "\n",
        "    # Load tokenizer info\n",
        "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
        "    tokenizer_info = None\n",
        "    if os.path.exists(tokenizer_path):\n",
        "        with open(tokenizer_path, 'r') as f:\n",
        "            tokenizer_info = json.load(f)\n",
        "\n",
        "    print(f\" Model loaded for inference:\")\n",
        "    print(f\"   Config: {config_dict}\")\n",
        "    if tokenizer_info:\n",
        "        print(f\"   Tokenizer: {tokenizer_info['tokenizer_name']}\")\n",
        "\n",
        "    return params, inference_config, tokenizer_info\n",
        "\n",
        "def create_inference_model(params, inference_config):\n",
        "    \"\"\"Create a model instance for inference (no training state).\"\"\"\n",
        "\n",
        "    # Create model with loaded config\n",
        "    model = GPT(\n",
        "        d_model=inference_config.d_model,\n",
        "        num_heads=inference_config.num_heads,\n",
        "        d_ff=inference_config.d_ff,\n",
        "        dropout_rate=inference_config.dropout_rate,\n",
        "        vocab_size=inference_config.vocab_size,\n",
        "        seq_len=inference_config.max_seq_len\n",
        "    )\n",
        "\n",
        "    # Create apply function with loaded parameters\n",
        "    def inference_apply(inputs, training=False):\n",
        "        return model.apply({'params': params}, inputs, training=training)\n",
        "\n",
        "    return model, inference_apply\n",
        "\n",
        "def load_and_setup_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Complete setup for inference - load everything and return ready-to-use functions.\"\"\"\n",
        "\n",
        "    # Load model components\n",
        "    params, inference_config, tokenizer_info = load_model_for_inference(model_dir, model_name)\n",
        "\n",
        "    # Create model and inference function\n",
        "    model, inference_apply = create_inference_model(params, inference_config)\n",
        "\n",
        "    # Setup tokenizer (you might want to load this separately)\n",
        "    if tokenizer_info and tokenizer_info.get('tokenizer_name') == 'gpt2':\n",
        "        from transformers import AutoTokenizer\n",
        "        inference_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        inference_tokenizer.pad_token = tokenizer_info.get('pad_token', '[PAD]')\n",
        "        print(f\" Tokenizer setup complete\")\n",
        "    else:\n",
        "        inference_tokenizer = None\n",
        "        print(\" No tokenizer info found, you'll need to setup tokenizer manually\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'apply_fn': inference_apply,\n",
        "        'params': params,\n",
        "        'config': inference_config,\n",
        "        'tokenizer': inference_tokenizer\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e6200ac8",
      "metadata": {
        "id": "e6200ac8"
      },
      "outputs": [],
      "source": [
        "# Advanced Text Generation with Top-K and Temperature Sampling\n",
        "import jax.random as random\n",
        "\n",
        "def top_k_sampling(logits, k=550, temperature=0.7, rng_key=None):\n",
        "    \"\"\"\n",
        "    Apply top-k sampling with temperature to logits.\n",
        "\n",
        "    Args:\n",
        "        logits: [vocab_size] array of logits\n",
        "        k: number of top candidates to keep\n",
        "        temperature: sampling temperature (lower = more deterministic)\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        sampled token index\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Get top-k indices and values\n",
        "    top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n",
        "\n",
        "    # Convert to probabilities\n",
        "    top_k_probs = jax.nn.softmax(top_k_logits)\n",
        "\n",
        "    # Sample from the top-k distribution\n",
        "    sampled_idx = jax.random.categorical(rng_key, jnp.log(top_k_probs))\n",
        "\n",
        "    # Return the actual token index\n",
        "    return top_k_indices[sampled_idx]\n",
        "\n",
        "def nucleus_sampling(logits, p=0.95, temperature=0.7, rng_key=None):\n",
        "    \"\"\"\n",
        "    Apply nucleus (top-p) sampling with temperature to logits.\n",
        "\n",
        "    Args:\n",
        "        logits: [vocab_size] array of logits\n",
        "        p: cumulative probability threshold\n",
        "        temperature: sampling temperature\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        sampled token index\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    # logits = logits / temperature\n",
        "\n",
        "    # Convert to probabilities and sort\n",
        "    probs = jax.nn.softmax(logits)\n",
        "    sorted_indices = jnp.argsort(probs)[::-1]  # Sort in descending order\n",
        "    sorted_probs = probs[sorted_indices]\n",
        "\n",
        "    # Find cumulative probabilities\n",
        "    cumsum_probs = jnp.cumsum(sorted_probs)\n",
        "\n",
        "    # Find the cutoff index where cumsum exceeds p\n",
        "    cutoff = jnp.searchsorted(cumsum_probs, p)\n",
        "    cutoff = jnp.maximum(cutoff, 1)  # Keep at least one token\n",
        "\n",
        "    # Keep only tokens within the nucleus\n",
        "    nucleus_indices = sorted_indices[:cutoff]\n",
        "    nucleus_probs = sorted_probs[:cutoff]\n",
        "    nucleus_probs = nucleus_probs / jnp.sum(nucleus_probs)  # Renormalize\n",
        "\n",
        "    # Sample from the nucleus\n",
        "    sampled_idx = jax.random.categorical(rng_key, jnp.log(nucleus_probs))\n",
        "\n",
        "    return nucleus_indices[sampled_idx]\n",
        "\n",
        "@jax.jit\n",
        "def generate_next_token(state, input_ids, temperature=1.0, top_k=50, use_nucleus=False, nucleus_p=0.9, rng_key=None):\n",
        "    \"\"\"\n",
        "    Generate the next token using the model.\n",
        "\n",
        "    Args:\n",
        "        state: training state with model parameters\n",
        "        input_ids: current sequence [batch_size, seq_len]\n",
        "        temperature: sampling temperature\n",
        "        top_k: number of top candidates for top-k sampling\n",
        "        use_nucleus: whether to use nucleus sampling instead of top-k\n",
        "        nucleus_p: probability threshold for nucleus sampling\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        next token index\n",
        "    \"\"\"\n",
        "    # Get model predictions\n",
        "    logits = state.apply_fn({'params': state.params}, input_ids, training=False)\n",
        "\n",
        "    # Take logits for the last position\n",
        "    next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
        "\n",
        "    # Apply sampling strategy\n",
        "    if use_nucleus:\n",
        "        next_token = nucleus_sampling(next_token_logits, p=nucleus_p, temperature=temperature, rng_key=rng_key)\n",
        "    else:\n",
        "        next_token = top_k_sampling(next_token_logits, k=top_k, temperature=temperature, rng_key=rng_key)\n",
        "\n",
        "    return next_token\n",
        "\n",
        "def generate_text(state, prompt, tokenizer, max_length=config.max_seq_len, temperature=0.7, top_k=500,\n",
        "                 use_nucleus=True, nucleus_p=0.95, seed=42, stop_at_eos=True, verbose=False):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model with advanced sampling.\n",
        "\n",
        "    Args:\n",
        "        state: training state with model parameters\n",
        "        prompt: input text string to start generation\n",
        "        tokenizer: tokenizer for encoding/decoding\n",
        "        max_length: maximum number of tokens to generate\n",
        "        temperature: sampling temperature (0.1 = conservative, 1.0 = balanced, 2.0 = creative)\n",
        "        top_k: number of top candidates for top-k sampling (lower = more focused)\n",
        "        use_nucleus: whether to use nucleus (top-p) sampling instead of top-k\n",
        "        nucleus_p: probability threshold for nucleus sampling (0.9 = balanced)\n",
        "        seed: random seed for reproducibility\n",
        "        stop_at_eos: whether to stop generation at EOS token\n",
        "        verbose: whether to print generation progress\n",
        "\n",
        "    Returns:\n",
        "        generated text string\n",
        "    \"\"\"\n",
        "    # Initialize random key\n",
        "    rng_key = jax.random.PRNGKey(seed)\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='np')\n",
        "    input_ids = jnp.array(input_ids).reshape(1, -1)  # [1, seq_len]\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   Generating text with:\")\n",
        "        print(f\"   Temperature: {temperature}\")\n",
        "        print(f\"   {'Nucleus (top-p)' if use_nucleus else 'Top-k'}: {nucleus_p if use_nucleus else top_k}\")\n",
        "        print(f\"   Max length: {max_length}\")\n",
        "        print(f\"   Prompt: '{prompt}'\")\n",
        "        print(\"    Generation:\")\n",
        "        print(prompt, end=\"\")\n",
        "\n",
        "    for i in range(max_length):\n",
        "        # Split the random key for this step\n",
        "        rng_key, step_key = jax.random.split(rng_key)\n",
        "\n",
        "        # Generate next token\n",
        "        next_token = generate_next_token(\n",
        "            state, input_ids,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            use_nucleus=use_nucleus,\n",
        "            nucleus_p=nucleus_p,\n",
        "            rng_key=step_key\n",
        "        )\n",
        "\n",
        "        # Convert to Python int\n",
        "        next_token = int(next_token)\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # Check for EOS token\n",
        "        if stop_at_eos and next_token == tokenizer.eos_token_id:\n",
        "            if verbose:\n",
        "                print(\" [EOS]\")\n",
        "            break\n",
        "\n",
        "        # Decode and print the new token if verbose\n",
        "        if verbose:\n",
        "            token_text = tokenizer.decode([next_token])\n",
        "            print(token_text, end=\"\", flush=True)\n",
        "\n",
        "        # Update input_ids for next iteration\n",
        "        next_token_array = jnp.array([[next_token]])\n",
        "        input_ids = jnp.concatenate([input_ids, next_token_array], axis=1)\n",
        "\n",
        "        # Truncate if sequence gets too long (to fit model's max_seq_len)\n",
        "        if input_ids.shape[1] > config.max_seq_len:\n",
        "            input_ids = input_ids[:, -config.max_seq_len:]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n Generation complete!\")\n",
        "\n",
        "    # Decode the full generated text\n",
        "    full_text = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "66c9c381",
      "metadata": {
        "id": "66c9c381"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing and collate function\n",
        "def collate(batch):\n",
        "    \"\"\"Collate function for DataLoader to handle TinyStories data.\"\"\"\n",
        "    # Extract text from batch\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Tokenize all texts\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        max_length=config.max_seq_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='np'\n",
        "    )\n",
        "\n",
        "    return encoded['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ae229e08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae229e08",
        "outputId": "8a722c9d-1dfc-442f-f6fa-37d74c9d1403"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7454,  2402,   257, ...,   655,   644,   284],\n",
              "       [ 7454,  2402,   257, ...,  7278, 31896,   503],\n",
              "       [   43, 10102,   290, ...,   318,   257,  2344],\n",
              "       ...,\n",
              "       [ 7454,  2402,   257, ..., 16896,   262,  1582],\n",
              "       [ 7454,  2402,   257, ...,  8072,   284,   307],\n",
              "       [ 7454,   612,   373, ...,  1611,   290,  1642]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate,\n",
        "    # num_workers=int(os.cpu_count() / 2)\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    # num_workers=int(os.cpu_count() / 2)\n",
        ")\n",
        "\n",
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1d4b1f67",
      "metadata": {
        "id": "1d4b1f67"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_to_file(text, step):\n",
        "\n",
        "    dir = './generated_texts'\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "    with open('generated_texts/{step}.txt', 'w') as f:\n",
        "        f.writelines(text + \"\\n\\n\")\n",
        "\n",
        "def train(resume_from_checkpoint=False, checkpoint_dir=\"./checkpoints\", save_every=1000):\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=\"smoljax-gpt\",\n",
        "        config={\n",
        "            \"vocab_size\": config.vocab_size,\n",
        "            \"max_seq_len\": config.max_seq_len,\n",
        "            \"d_model\": config.d_model,\n",
        "            \"num_layers\": config.num_layers,\n",
        "            \"num_heads\": config.num_heads,\n",
        "            \"d_ff\": config.d_ff,\n",
        "            \"dropout_rate\": config.dropout_rate,\n",
        "            \"learning_rate\": config.lr,\n",
        "            \"warmup_steps\": config.warmup_steps,\n",
        "            \"total_steps\": config.total_steps,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"num_epochs\": config.num_epochs,\n",
        "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
        "            \"save_every\": save_every,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate,\n",
        "        # num_workers=int(os.cpu_count() / 2)\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate,\n",
        "        # num_workers=int(os.cpu_count() / 2)\n",
        "    )\n",
        "\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    train_state = create_train_state(rng, config)\n",
        "\n",
        "    # Initialize variables\n",
        "    start_step = 0\n",
        "    tokens_processed = 0\n",
        "\n",
        "    # Try to resume from checkpoint if requested\n",
        "    if resume_from_checkpoint:\n",
        "        print(\"Checking for existing checkpoints...\")\n",
        "        available_checkpoints = get_checkpoint_info(checkpoint_dir)\n",
        "\n",
        "        if available_checkpoints:\n",
        "            print(f\"Found checkpoints at steps: {available_checkpoints}\")\n",
        "            restored_state, start_step = load_checkpoint(checkpoint_dir, train_state)\n",
        "\n",
        "            if restored_state is not None:\n",
        "                train_state = restored_state\n",
        "                print(f\"Resuming training from step {start_step}\")\n",
        "\n",
        "                # Estimate tokens processed (rough approximation)\n",
        "                tokens_processed = start_step * config.batch_size * config.max_seq_len\n",
        "                print(f\"Estimated tokens processed so far: {tokens_processed:,}\")\n",
        "            else:\n",
        "                print(\"Failed to load checkpoint, starting from scratch\")\n",
        "        else:\n",
        "            print(\"No checkpoints found, starting fresh training\")\n",
        "\n",
        "    # Log model summary to wandb\n",
        "    total_params = sum([param.size for param in jax.tree_util.tree_leaves(train_state.params)])\n",
        "    wandb.log({\n",
        "        \"model/total_parameters\": total_params,\n",
        "        \"model/model_size_mb\": total_params * 4 / (1024**2),  # float32 = 4 bytes\n",
        "        \"checkpoint/resume_from\": start_step\n",
        "    })\n",
        "\n",
        "    # Log detailed training summary\n",
        "    log_training_summary(train_state, config, total_params, tokens_processed)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = config.num_epochs\n",
        "    state = train_state.replace(step=start_step)  # Set correct step for LR schedule\n",
        "    global_step = start_step\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        train_losses = []\n",
        "        train_grad_norms = []\n",
        "\n",
        "        # Create data iterator\n",
        "        train_data_iterator = iter(train_loader)\n",
        "\n",
        "\n",
        "        # Progress bar tracks optimization steps, not data batches\n",
        "        pbar = tqdm(range(config.total_steps), desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for step_idx in pbar:\n",
        "\n",
        "            # Initialize accumulation variables for this step\n",
        "            step_losses = []\n",
        "            step_grad_norms = []\n",
        "            rng, step_key = jax.random.split(rng)\n",
        "\n",
        "            # Gradient accumulation loop\n",
        "            for micro_step in range(config.gradient_accumulation_steps):\n",
        "                try:\n",
        "                    batch = next(train_data_iterator)\n",
        "                except StopIteration:\n",
        "                    train_data_iterator = iter(train_loader)\n",
        "                    batch = next(train_data_iterator)\n",
        "                print(\"\\nRunning gradient accum step: \", micro_step)\n",
        "                # Convert to JAX array\n",
        "                batch = jnp.array(batch)\n",
        "\n",
        "                # Count tokens (excluding padding)\n",
        "                batch_tokens = jnp.sum(batch != tokenizer.pad_token_id)\n",
        "                tokens_processed += int(batch_tokens)\n",
        "\n",
        "                state, loss, grad_norm = train_step_accum(state, batch, step_key)\n",
        "                step_losses.append(float(loss))\n",
        "                # print(grad_norm)\n",
        "                step_grad_norms.append(grad_norm)\n",
        "\n",
        "            # Average the losses and grad norms from accumulation steps\n",
        "            avg_step_loss = np.mean(step_losses)\n",
        "            avg_step_grad_norm = np.mean(step_grad_norms) if step_grad_norms else 0.0\n",
        "\n",
        "            train_losses.append(avg_step_loss)\n",
        "            train_grad_norms.append(avg_step_grad_norm)\n",
        "\n",
        "            # if grad_norm is not None:\n",
        "            #         step_grad_norms.append(float(avg_step_grad_norm))\n",
        "            global_step += 1\n",
        "\n",
        "            # Get current learning rate\n",
        "            print(state.opt_state[2])\n",
        "            current_lr = float(state.opt_state[2].hyperparams['learning_rate'])\n",
        "\n",
        "            # Log training metrics to wandb\n",
        "            wandb.log({\n",
        "                \"train/loss\": avg_step_loss,\n",
        "                \"train/grad_norm\": avg_step_grad_norm,\n",
        "                \"train/learning_rate\": current_lr,\n",
        "                \"train/tokens_processed\": tokens_processed,\n",
        "                \"train/epoch\": epoch + 1,\n",
        "                \"train/batch_size\": config.batch_size,\n",
        "                \"train/step\": global_step\n",
        "            }, step=global_step)\n",
        "\n",
        "            # Save checkpoint every save_every steps\n",
        "            if global_step % save_every == 0:\n",
        "                save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "                # Also save model-only version for inference\n",
        "                save_model_for_inference(\n",
        "                    state,\n",
        "                    model_dir=\"./saved_models\",\n",
        "                    model_name=f\"smoljax_gpt_step_{global_step}\"\n",
        "                )\n",
        "\n",
        "                # Log checkpoint info to wandb\n",
        "                wandb.log({\n",
        "                    \"checkpoint/last_saved_step\": global_step,\n",
        "                    \"checkpoint/tokens_at_save\": tokens_processed\n",
        "                }, step=global_step)\n",
        "\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Step {global_step}/{config.total_steps} [Train]\")\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{avg_step_loss:.4f}\",\n",
        "                \"grad_norm\": f\"{avg_step_grad_norm:.4f}\",\n",
        "                \"lr\": f\"{current_lr:.6f}\",\n",
        "                \"tokens\": f\"{tokens_processed:,}\",\n",
        "                \"micro_batches\": f\"{config.gradient_accumulation_steps}\"\n",
        "            })\n",
        "\n",
        "            # Break if we've reached the total steps\n",
        "            if global_step >= config.total_steps:\n",
        "                print(f\"\\nReached maximum steps ({config.total_steps}), stopping training...\")\n",
        "                break\n",
        "\n",
        "        # Break out of epoch loop if we've reached max steps\n",
        "        if global_step >= config.total_steps:\n",
        "            break\n",
        "\n",
        "        # Validation\n",
        "        val_losses = []\n",
        "        val_accs = []\n",
        "\n",
        "        # Simple validation without iterator complexity\n",
        "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "        for batch in pbar:\n",
        "            batch = jnp.array(batch)\n",
        "            loss, acc = eval_step(state, batch)\n",
        "            val_losses.append(float(loss))\n",
        "            # val_accs.append(float(acc))\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{loss:.4f}\",\n",
        "                # \"acc\": f\"{acc:.4f}\"\n",
        "            })\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_train_grad_norm = np.mean(train_grad_norms)\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        # avg_val_acc = np.mean(val_accs)\n",
        "\n",
        "        # Log epoch metrics to wandb\n",
        "        wandb.log({\n",
        "            \"epoch/train_loss\": avg_train_loss,\n",
        "            \"epoch/val_loss\": avg_val_loss,\n",
        "            # \"epoch/val_accuracy\": avg_val_acc,\n",
        "            \"epoch/train_grad_norm\": avg_train_grad_norm,\n",
        "            \"epoch/epoch\": epoch + 1\n",
        "        }, step=global_step)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Grad Norm: {avg_train_grad_norm:.4f}\")\n",
        "        # print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
        "        print(f\"  Tokens Processed: {tokens_processed:,}\")\n",
        "        print(f\"  Global Step: {global_step}\")\n",
        "\n",
        "        # Save checkpoint at end of each epoch\n",
        "        save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "    # Save final checkpoint\n",
        "    print(\"\\n Saving final checkpoint...\")\n",
        "    save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "    # Generate some text\n",
        "    print(\"\\nGenerating text...\")\n",
        "    generated = generate_text(state, \"The future of artificial intelligence\", tokenizer, max_length=50)\n",
        "    print(f\"Generated: {generated}\")\n",
        "    save_to_file(generated, global_step)\n",
        "\n",
        "    # Log final generation to wandb\n",
        "    wandb.log({\n",
        "        \"generation/sample_text\": generated,\n",
        "        \"generation/prompt\": \"The future of artificial intelligence\",\n",
        "        \"training/final_step\": global_step,\n",
        "        \"training/final_tokens\": tokens_processed\n",
        "    })\n",
        "\n",
        "    # Log final training summary\n",
        "    log_training_summary(state, config, total_params, tokens_processed)\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "22f407e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "22f407e8",
        "outputId": "fe4fdee3-520a-4a60-ccb1-39205130882f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250923_151039-3x7yoop5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rentio/smoljax-gpt/runs/3x7yoop5' target=\"_blank\">vague-aardvark-23</a></strong> to <a href='https://wandb.ai/rentio/smoljax-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rentio/smoljax-gpt' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rentio/smoljax-gpt/runs/3x7yoop5' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt/runs/3x7yoop5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1 [Train]:   0%|          | 0/20000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running gradient accum step:  0\n",
            "\n",
            "Running gradient accum step:  1\n",
            "\n",
            "Running gradient accum step:  2\n",
            "\n",
            "Running gradient accum step:  3\n",
            "\n",
            "Running gradient accum step:  4\n",
            "\n",
            "Running gradient accum step:  5\n",
            "\n",
            "Running gradient accum step:  6\n",
            "\n",
            "Running gradient accum step:  7\n",
            "\n",
            "Running gradient accum step:  8\n",
            "\n",
            "Running gradient accum step:  9\n",
            "\n",
            "Running gradient accum step:  10\n",
            "\n",
            "Running gradient accum step:  11\n",
            "\n",
            "Running gradient accum step:  12\n",
            "\n",
            "Running gradient accum step:  13\n",
            "\n",
            "Running gradient accum step:  14\n",
            "\n",
            "Running gradient accum step:  15\n",
            "\n",
            "Running gradient accum step:  16\n",
            "\n",
            "Running gradient accum step:  17\n",
            "\n",
            "Running gradient accum step:  18\n",
            "\n",
            "Running gradient accum step:  19\n",
            "\n",
            "Running gradient accum step:  20\n",
            "\n",
            "Running gradient accum step:  21\n",
            "\n",
            "Running gradient accum step:  22\n",
            "\n",
            "Running gradient accum step:  23\n",
            "\n",
            "Running gradient accum step:  24\n",
            "\n",
            "Running gradient accum step:  25\n",
            "\n",
            "Running gradient accum step:  26\n",
            "\n",
            "Running gradient accum step:  27\n",
            "\n",
            "Running gradient accum step:  28\n",
            "\n",
            "Running gradient accum step:  29\n",
            "\n",
            "Running gradient accum step:  30\n",
            "\n",
            "Running gradient accum step:  31\n",
            "\n",
            "Running gradient accum step:  32\n",
            "\n",
            "Running gradient accum step:  33\n",
            "\n",
            "Running gradient accum step:  34\n",
            "\n",
            "Running gradient accum step:  35\n",
            "\n",
            "Running gradient accum step:  36\n",
            "\n",
            "Running gradient accum step:  37\n",
            "\n",
            "Running gradient accum step:  38\n",
            "\n",
            "Running gradient accum step:  39\n",
            "\n",
            "Running gradient accum step:  40\n",
            "\n",
            "Running gradient accum step:  41\n",
            "\n",
            "Running gradient accum step:  42\n",
            "\n",
            "Running gradient accum step:  43\n",
            "\n",
            "Running gradient accum step:  44\n",
            "\n",
            "Running gradient accum step:  45\n",
            "\n",
            "Running gradient accum step:  46\n",
            "\n",
            "Running gradient accum step:  47\n",
            "\n",
            "Running gradient accum step:  48\n",
            "\n",
            "Running gradient accum step:  49\n",
            "\n",
            "Running gradient accum step:  50\n",
            "\n",
            "Running gradient accum step:  51\n",
            "\n",
            "Running gradient accum step:  52\n",
            "\n",
            "Running gradient accum step:  53\n",
            "\n",
            "Running gradient accum step:  54\n",
            "\n",
            "Running gradient accum step:  55\n",
            "\n",
            "Running gradient accum step:  56\n",
            "\n",
            "Running gradient accum step:  57\n",
            "\n",
            "Running gradient accum step:  58\n",
            "\n",
            "Running gradient accum step:  59\n",
            "\n",
            "Running gradient accum step:  60\n",
            "\n",
            "Running gradient accum step:  61\n",
            "\n",
            "Running gradient accum step:  62\n",
            "\n",
            "Running gradient accum step:  63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1 [Train]:   0%|          | 0/20000 [02:53<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ScaleByScheduleState(count=Array(1, dtype=int32))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ScaleByScheduleState' object has no attribute 'hyperparams'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3364925475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-732297526.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(resume_from_checkpoint, checkpoint_dir, save_every)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# Get current learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Log training metrics to wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ScaleByScheduleState' object has no attribute 'hyperparams'"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state"
      ],
      "metadata": {
        "id": "lv04guk25onN"
      },
      "id": "lv04guk25onN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f8e0fc675fa4ab7b55078d65bf0b3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_977918c696b44e2b850b99e943f8337a",
              "IPY_MODEL_6cfee82ae404423ba0bf4edead5a446e",
              "IPY_MODEL_299804612e1d42ff8576d65336b87489",
              "IPY_MODEL_7edda5d229d24813ab76d26681756ea5",
              "IPY_MODEL_88094f7cd20f4ae6aeab0655fd6e0a16"
            ],
            "layout": "IPY_MODEL_392cef72dc0b48d29b46d555570ebf7a"
          }
        },
        "977918c696b44e2b850b99e943f8337a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fef56ad4eed4071af3cec0c37d645cb",
            "placeholder": "​",
            "style": "IPY_MODEL_2d39b21fefca42c09f297c0698f1c218",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6cfee82ae404423ba0bf4edead5a446e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0db380497d9e4996978568642cc65cb1",
            "placeholder": "​",
            "style": "IPY_MODEL_69c861a9812c48709f5476dc7acba4c8",
            "value": ""
          }
        },
        "299804612e1d42ff8576d65336b87489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c9efd464006840148dd16349f2e1f565",
            "style": "IPY_MODEL_1dbd561c6d524d96aa63645835d7b120",
            "value": true
          }
        },
        "7edda5d229d24813ab76d26681756ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_edecfa1aa55d49248b16a48d575e5608",
            "style": "IPY_MODEL_e71900a34d6446a39c2dbc27f3d6e751",
            "tooltip": ""
          }
        },
        "88094f7cd20f4ae6aeab0655fd6e0a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52f4b93f2224761966ccff094b1b77d",
            "placeholder": "​",
            "style": "IPY_MODEL_53df73a036174709aaf37eb119475df1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "392cef72dc0b48d29b46d555570ebf7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7fef56ad4eed4071af3cec0c37d645cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d39b21fefca42c09f297c0698f1c218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0db380497d9e4996978568642cc65cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c861a9812c48709f5476dc7acba4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9efd464006840148dd16349f2e1f565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbd561c6d524d96aa63645835d7b120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edecfa1aa55d49248b16a48d575e5608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e71900a34d6446a39c2dbc27f3d6e751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b52f4b93f2224761966ccff094b1b77d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53df73a036174709aaf37eb119475df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}