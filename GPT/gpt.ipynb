{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "151fd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.training import checkpoints\n",
    "from flax import struct\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Any\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "502276f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    grad_accum: Any = None\n",
    "    accum_step: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cafc054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2fa95aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading TinyStories dataset from Huggingface\n",
    "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d012c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for model parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = tokenizer.vocab_size\n",
    "    max_seq_len: int = 1024\n",
    "    d_model: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    d_ff: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    lr: float = 6e-4\n",
    "    warmup_steps: int = 700\n",
    "    total_steps: int = 20000\n",
    "    batch_size: int = 64\n",
    "    required_bsz: int = 524288\n",
    "    gradient_accumulation_steps: int = int(required_bsz // (batch_size * max_seq_len))\n",
    "    mixed_precision: bool = False\n",
    "    num_epochs: int = 1\n",
    "    \n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "16494b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "        self.d_Q = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_K = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_V = nn.Dense(features=self.head_size, use_bias=False)\n",
    "        self.d_O = nn.Dense(features=self.d_model, use_bias=False)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "            B,T,C = x.shape\n",
    "            query = self.d_Q(x)\n",
    "            key = self.d_K(x)\n",
    "            value = self.d_V(x)\n",
    "            \n",
    "            weights = jnp.matmul(query, key.transpose(0,2, 1)) * (key.shape[-1] ** -0.5)\n",
    "            mask = jnp.tril(jnp.ones((T,T)))\n",
    "            mask = jnp.where(mask==0, -1e9, 1.0)\n",
    "            weights = weights * mask\n",
    "            weights = nn.softmax(weights, axis=-1)\n",
    "            out = jnp.matmul(weights, value)\n",
    "            out = self.d_O(out)\n",
    "            out = self.dropout(out, deterministic=not training)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9c23bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jnp.tril(jnp.ones((128,128)))\n",
    "mask = jnp.where(mask==0, -1e9, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fa5c50c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.e+00, -1.e+09, -1.e+09, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00, -1.e+09, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ..., -1.e+09, -1.e+09, -1.e+09],\n",
       "       ...,\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00, -1.e+09, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00,  1.e+00, -1.e+09],\n",
       "       [ 1.e+00,  1.e+00,  1.e+00, ...,  1.e+00,  1.e+00,  1.e+00]],      dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6eb96ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.heads = [Attention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "        self.linear = nn.Dense(features=self.d_model)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "60072f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_attention():\n",
    "    attn = MHA()\n",
    "    x = jnp.ones((2, 128, config.d_model))\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = attn.init(rng, x, training=True)\n",
    "    out = attn.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == x.shape, f\"Expected {x.shape}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "af9d5276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1fd804b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.fc1 = nn.Dense(features=self.d_ff)\n",
    "        self.fc2 = nn.Dense(features=self.d_model)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "eeb03796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_mlp():\n",
    "    mlp = MLP()\n",
    "    x = jnp.ones((2, 128, config.d_model))\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = mlp.init(rng, x, training=True)\n",
    "    out = mlp.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == x.shape, f\"Expected {x.shape}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d8ccb08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c1b2a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    \n",
    "    def setup(self):\n",
    "        self.attention = MHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm()\n",
    "        self.ln2 = nn.LayerNorm()\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        x = x + self.attention(self.ln1(x), training)\n",
    "        x = x + self.mlp(self.ln2(x), training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c83412e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    vocab_size: int = config.vocab_size\n",
    "    seq_len: int = config.max_seq_len\n",
    "    \n",
    "    def setup(self):\n",
    "        self.embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model)\n",
    "        self.positional_embedding = self.param(\n",
    "            \"positional_embeddings\",  # name\n",
    "            lambda key: jax.random.normal(key, (1, self.seq_len, self.d_model)) * 0.02\n",
    "        )\n",
    "        self.decoder = [TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(config.num_layers)]\n",
    "        self.linear_out = nn.Dense(features=self.vocab_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        B,T = x.shape\n",
    "        embeds = self.embedding_table(x)  # (B,T,d_model)\n",
    "        C = embeds.shape[-1]\n",
    "        pos_embeds = self.positional_embedding[:, :T, :]  # (1,T,d_model)\n",
    "        x = embeds + pos_embeds  # (B,T,d_model)\n",
    "        pad_mask = (x != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "        x = x * pad_mask\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, training=training)\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b6eb78ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved to model_summary.txt\n",
      "Total Parameters: 247,992,913\n",
      "Model size: ~946.0 MB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to inspect the model summary like torchsummary\n",
    "\n",
    "from flax.linen import tabulate\n",
    "import jax\n",
    "\n",
    "# Initialize model\n",
    "model = GPT()\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((1, 128), dtype=jnp.int32)  # Dummy input for tabulation\n",
    "\n",
    "# Tabulate the model structure\n",
    "tabulate_fn = tabulate(model, key, console_kwargs={'width': 120})\n",
    "\n",
    "# Count total parameters\n",
    "params = model.init(key, x)['params']\n",
    "total_params = sum(jax.tree_util.tree_leaves(jax.tree.map(lambda arr: arr.size, params)))\n",
    "\n",
    "# Get raw summary and clean ANSI codes\n",
    "raw_summary = tabulate_fn(x, training=True)\n",
    "# Remove ANSI color codes for clean logging\n",
    "clean_summary = re.sub(r'\\x1b\\[[0-9;]*m', '', raw_summary)\n",
    "\n",
    "# Save to log file with clean formatting\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"GPT MODEL ARCHITECTURE SUMMARY\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"Model Configuration:\\n\")\n",
    "    f.write(f\"  - Vocabulary Size: {config.vocab_size:,}\\n\")\n",
    "    f.write(f\"  - Max Sequence Length: {config.max_seq_len}\\n\")\n",
    "    f.write(f\"  - Model Dimension: {config.d_model}\\n\")\n",
    "    f.write(f\"  - Number of Layers: {config.num_layers}\\n\")\n",
    "    f.write(f\"  - Number of Heads: {config.num_heads}\\n\")\n",
    "    f.write(f\"  - Feed Forward Dimension: {config.d_ff}\\n\")\n",
    "    f.write(f\"  - Dropout Rate: {config.dropout_rate}\\n\\n\")\n",
    "    f.write(\"Detailed Layer Information:\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(clean_summary)\n",
    "\n",
    "print(f\"Model summary saved to model_summary.txt\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / (1024**2):.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "20290ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt():\n",
    "    model = GPT()\n",
    "    x = jnp.ones((2, 128), dtype=jnp.int32)\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    params = model.init(rng, x, training=True)\n",
    "    out = model.apply(params, x, training=True, rngs={'dropout': jax.random.PRNGKey(1), \"positional_embeddings\": jax.random.PRNGKey(2)})\n",
    "    \n",
    "     # Check shape\n",
    "    assert out.shape == (2, 128, config.vocab_size), f\"Expected {(2, 128, config.vocab_size)}, got {out.shape}\"\n",
    "        # assert out.shape == x.shape\n",
    "    print(\"Shape test passed!\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "047005f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test passed! (2, 128, 50257)\n"
     ]
    }
   ],
   "source": [
    "test_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "57b39759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_rate_schedule():\n",
    "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
    "    config = GPTConfig()\n",
    "    def schedule(step):\n",
    "        # Linear warmup\n",
    "        warmup_ratio = jnp.minimum(1.0, step / config.warmup_steps)\n",
    "        # Cosine decay after warmup\n",
    "        decay_ratio = jnp.maximum(0.0, (step - config.warmup_steps) / (config.total_steps - config.warmup_steps))\n",
    "        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * decay_ratio))\n",
    "        return config.lr * warmup_ratio * cosine_decay\n",
    "\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "687a1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    labels = labels[:, 1:]\n",
    "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
    "    \n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "    model = GPT()\n",
    "    \n",
    "    # Initialize parameters\n",
    "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
    "    params = model.init(rng, dummy_input)['params']\n",
    "    \n",
    "    # Create learning rate schedule\n",
    "    lr_schedule = create_learning_rate_schedule()\n",
    "    \n",
    "    # Create optimizer\n",
    "    tx = optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.95,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True)\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "    \n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    \n",
    "    # Compute gradient norm for logging\n",
    "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
    "    \n",
    "    # Update the parameters\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "35c7ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_accum(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True)\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "\n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    \n",
    "    if state.grad_accum is None:\n",
    "        state = state.replace(\n",
    "            grad_accum=jax.tree_util.tree_map(jnp.zeros_like, grads),\n",
    "            accum_step=0,\n",
    "        )\n",
    "    new_accum = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, state.grad_accum, grads)\n",
    "    new_step = state.accum_step + 1\n",
    "    \n",
    "    def apply_update(_):\n",
    "        mean_grads = jax.tree_util.tree_map(lambda g: g / config.gradient_accumulation_steps, new_accum)\n",
    "        new_state = state.apply_gradients(grads = mean_grads)\n",
    "        return new_state.replace(grad_accum=None, accum_step=0), mean_grads\n",
    "    \n",
    "    def carry_forward(_):\n",
    "        return state.replace(grad_accum=new_accum, accum_step=new_step), None\n",
    "        \n",
    "        \n",
    "    state, grads = jax.lax.cond(new_step == config.gradient_accumulation_steps, apply_update, carry_forward, operand=None)\n",
    "    \n",
    "    return state, loss, grads\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f95fc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compiled evaluation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    \"\"\"Single evaluation step.\"\"\"\n",
    "    logits = state.apply_fn({'params': state.params}, batch, training=False)\n",
    "    loss = compute_ce_loss(logits, batch)\n",
    "    \n",
    "    return loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "12a9d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized prediction function using vmap\n",
    "@jax.jit\n",
    "def predict_batch(state, batch):\n",
    "    \"\"\"Generate predictions for a batch using vmap.\"\"\"\n",
    "    return state.apply_fn({'params': state.params}, batch, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "63c34940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create wandb summary table\n",
    "def log_training_summary(state, config, total_params, tokens_processed):\n",
    "    \"\"\"Log a comprehensive training summary to wandb.\"\"\"\n",
    "    \n",
    "    # Create a summary table (ensure all values are strings for wandb compatibility)\n",
    "    summary_data = [\n",
    "        [\"Model\", \"SmolJAX GPT\"],\n",
    "        [\"Total Parameters\", f\"{total_params:,}\"],\n",
    "        [\"Model Size (MB)\", f\"{total_params * 4 / (1024**2):.1f}\"],\n",
    "        [\"Vocabulary Size\", f\"{config.vocab_size:,}\"],\n",
    "        [\"Max Sequence Length\", f\"{config.max_seq_len}\"],\n",
    "        [\"Model Dimension\", f\"{config.d_model}\"],\n",
    "        [\"Number of Layers\", f\"{config.num_layers}\"],\n",
    "        [\"Number of Heads\", f\"{config.num_heads}\"],\n",
    "        [\"Feed Forward Dimension\", f\"{config.d_ff}\"],\n",
    "        [\"Dropout Rate\", f\"{config.dropout_rate}\"],\n",
    "        [\"Learning Rate\", f\"{config.lr}\"],\n",
    "        [\"Batch Size\", f\"{config.batch_size}\"],\n",
    "        [\"Total Epochs\", f\"{config.num_epochs}\"],\n",
    "        [\"Tokens Processed\", f\"{tokens_processed:,}\"],\n",
    "        [\"Training Step\", f\"{int(state.step)}\"]\n",
    "    ]\n",
    "    \n",
    "    # Create wandb table\n",
    "    table = wandb.Table(\n",
    "        columns=[\"Metric\", \"Value\"],\n",
    "        data=summary_data\n",
    "    )\n",
    "    \n",
    "    wandb.log({\"training_summary\": table})\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e4a98639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management functions\n",
    "def save_checkpoint(state, step, checkpoint_dir=\"./checkpoints\", keep=5):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    checkpoints.save_checkpoint(\n",
    "        ckpt_dir=checkpoint_dir,\n",
    "        target=state,\n",
    "        step=step,\n",
    "        keep=keep,  # Keep only the last 5 checkpoints\n",
    "        overwrite=True\n",
    "    )\n",
    "    \n",
    "    print(f\"💾 Checkpoint saved at step {step} in {checkpoint_dir}\")\n",
    "    \n",
    "    # Log to wandb if available\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"checkpoint/step\": step,\n",
    "            \"checkpoint/saved\": 1\n",
    "        }, step=step)\n",
    "\n",
    "def load_checkpoint(checkpoint_dir=\"./checkpoints\", state=None):\n",
    "    \"\"\"Load the latest checkpoint.\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        print(f\"❌ Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        return None, 0\n",
    "    \n",
    "    # Check if there are any checkpoints\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
    "    if not checkpoint_files:\n",
    "        print(f\"❌ No checkpoints found in {checkpoint_dir}\")\n",
    "        return None, 0\n",
    "    \n",
    "    try:\n",
    "        # Load the latest checkpoint\n",
    "        restored_state = checkpoints.restore_checkpoint(\n",
    "            ckpt_dir=checkpoint_dir,\n",
    "            target=state\n",
    "        )\n",
    "        \n",
    "        # Get the step number from the checkpoint\n",
    "        latest_step = checkpoints.latest_checkpoint(checkpoint_dir)\n",
    "        if latest_step:\n",
    "            step = int(latest_step.split('_')[-1])\n",
    "            print(f\"✅ Checkpoint loaded from step {step}\")\n",
    "            return restored_state, step\n",
    "        else:\n",
    "            print(f\"❌ Could not determine step from checkpoint\")\n",
    "            return None, 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading checkpoint: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def get_checkpoint_info(checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Get information about available checkpoints.\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return []\n",
    "    \n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
    "    checkpoint_steps = []\n",
    "    \n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            step = int(file.split('_')[-1])\n",
    "            checkpoint_steps.append(step)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return sorted(checkpoint_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "522e41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-only saving/loading functions for inference\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def save_model_for_inference(state, model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Save only model parameters and config for inference (much smaller files).\"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save just the parameters (no optimizer state)\n",
    "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
    "    with open(params_path, 'wb') as f:\n",
    "        pickle.dump(state.params, f)\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
    "    config_dict = {\n",
    "        \"vocab_size\": config.vocab_size,\n",
    "        \"max_seq_len\": config.max_seq_len,\n",
    "        \"d_model\": config.d_model,\n",
    "        \"num_layers\": config.num_layers,\n",
    "        \"num_heads\": config.num_heads,\n",
    "        \"d_ff\": config.d_ff,\n",
    "        \"dropout_rate\": config.dropout_rate\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer info\n",
    "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
    "    tokenizer_info = {\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"pad_token\": tokenizer.pad_token,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token\": tokenizer.eos_token,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    with open(tokenizer_path, 'w') as f:\n",
    "        json.dump(tokenizer_info, f, indent=2)\n",
    "\n",
    "    print(f\"   Model saved for inference:\")\n",
    "    print(f\"   Parameters: {params_path}\")\n",
    "    print(f\"   Config: {config_path}\")\n",
    "    print(f\"   Tokenizer info: {tokenizer_path}\")\n",
    "    \n",
    "    # Calculate file sizes\n",
    "    params_size = os.path.getsize(params_path) / (1024**2)  # MB\n",
    "    print(f\"   Model size: {params_size:.1f} MB\")\n",
    "    \n",
    "    return params_path, config_path, tokenizer_path\n",
    "\n",
    "def load_model_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Load model parameters and config for inference.\"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Recreate config object\n",
    "    inference_config = GPTConfig(**config_dict)\n",
    "    \n",
    "    # Load parameters\n",
    "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
    "    if not os.path.exists(params_path):\n",
    "        raise FileNotFoundError(f\"Parameters file not found: {params_path}\")\n",
    "    \n",
    "    with open(params_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    # Load tokenizer info\n",
    "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
    "    tokenizer_info = None\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            tokenizer_info = json.load(f)\n",
    "\n",
    "    print(f\" Model loaded for inference:\")\n",
    "    print(f\"   Config: {config_dict}\")\n",
    "    if tokenizer_info:\n",
    "        print(f\"   Tokenizer: {tokenizer_info['tokenizer_name']}\")\n",
    "    \n",
    "    return params, inference_config, tokenizer_info\n",
    "\n",
    "def create_inference_model(params, inference_config):\n",
    "    \"\"\"Create a model instance for inference (no training state).\"\"\"\n",
    "    \n",
    "    # Create model with loaded config\n",
    "    model = GPT(\n",
    "        d_model=inference_config.d_model,\n",
    "        num_heads=inference_config.num_heads,\n",
    "        d_ff=inference_config.d_ff,\n",
    "        dropout_rate=inference_config.dropout_rate,\n",
    "        vocab_size=inference_config.vocab_size,\n",
    "        seq_len=inference_config.max_seq_len\n",
    "    )\n",
    "    \n",
    "    # Create apply function with loaded parameters\n",
    "    def inference_apply(inputs, training=False):\n",
    "        return model.apply({'params': params}, inputs, training=training)\n",
    "    \n",
    "    return model, inference_apply\n",
    "\n",
    "def load_and_setup_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Complete setup for inference - load everything and return ready-to-use functions.\"\"\"\n",
    "    \n",
    "    # Load model components\n",
    "    params, inference_config, tokenizer_info = load_model_for_inference(model_dir, model_name)\n",
    "    \n",
    "    # Create model and inference function\n",
    "    model, inference_apply = create_inference_model(params, inference_config)\n",
    "    \n",
    "    # Setup tokenizer (you might want to load this separately)\n",
    "    if tokenizer_info and tokenizer_info.get('tokenizer_name') == 'gpt2':\n",
    "        from transformers import AutoTokenizer\n",
    "        inference_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        inference_tokenizer.pad_token = tokenizer_info.get('pad_token', '[PAD]')\n",
    "        print(f\" Tokenizer setup complete\")\n",
    "    else:\n",
    "        inference_tokenizer = None\n",
    "        print(\" No tokenizer info found, you'll need to setup tokenizer manually\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'apply_fn': inference_apply,\n",
    "        'params': params,\n",
    "        'config': inference_config,\n",
    "        'tokenizer': inference_tokenizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e6200ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Text Generation with Top-K and Temperature Sampling\n",
    "import jax.random as random\n",
    "\n",
    "def top_k_sampling(logits, k=550, temperature=0.7, rng_key=None):\n",
    "    \"\"\"\n",
    "    Apply top-k sampling with temperature to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: [vocab_size] array of logits\n",
    "        k: number of top candidates to keep\n",
    "        temperature: sampling temperature (lower = more deterministic)\n",
    "        rng_key: PRNG key for randomness\n",
    "    \n",
    "    Returns:\n",
    "        sampled token index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Get top-k indices and values\n",
    "    top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    top_k_probs = jax.nn.softmax(top_k_logits)\n",
    "    \n",
    "    # Sample from the top-k distribution\n",
    "    sampled_idx = jax.random.categorical(rng_key, jnp.log(top_k_probs))\n",
    "    \n",
    "    # Return the actual token index\n",
    "    return top_k_indices[sampled_idx]\n",
    "\n",
    "def nucleus_sampling(logits, p=0.95, temperature=0.7, rng_key=None):\n",
    "    \"\"\"\n",
    "    Apply nucleus (top-p) sampling with temperature to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: [vocab_size] array of logits\n",
    "        p: cumulative probability threshold\n",
    "        temperature: sampling temperature\n",
    "        rng_key: PRNG key for randomness\n",
    "    \n",
    "    Returns:\n",
    "        sampled token index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    # logits = logits / temperature\n",
    "    \n",
    "    # Convert to probabilities and sort\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    sorted_indices = jnp.argsort(probs)[::-1]  # Sort in descending order\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    \n",
    "    # Find cumulative probabilities\n",
    "    cumsum_probs = jnp.cumsum(sorted_probs)\n",
    "    \n",
    "    # Find the cutoff index where cumsum exceeds p\n",
    "    cutoff = jnp.searchsorted(cumsum_probs, p)\n",
    "    cutoff = jnp.maximum(cutoff, 1)  # Keep at least one token\n",
    "    \n",
    "    # Keep only tokens within the nucleus\n",
    "    nucleus_indices = sorted_indices[:cutoff]\n",
    "    nucleus_probs = sorted_probs[:cutoff]\n",
    "    nucleus_probs = nucleus_probs / jnp.sum(nucleus_probs)  # Renormalize\n",
    "    \n",
    "    # Sample from the nucleus\n",
    "    sampled_idx = jax.random.categorical(rng_key, jnp.log(nucleus_probs))\n",
    "    \n",
    "    return nucleus_indices[sampled_idx]\n",
    "\n",
    "@jax.jit\n",
    "def generate_next_token(state, input_ids, temperature=1.0, top_k=50, use_nucleus=False, nucleus_p=0.9, rng_key=None):\n",
    "    \"\"\"\n",
    "    Generate the next token using the model.\n",
    "    \n",
    "    Args:\n",
    "        state: training state with model parameters\n",
    "        input_ids: current sequence [batch_size, seq_len]\n",
    "        temperature: sampling temperature\n",
    "        top_k: number of top candidates for top-k sampling\n",
    "        use_nucleus: whether to use nucleus sampling instead of top-k\n",
    "        nucleus_p: probability threshold for nucleus sampling\n",
    "        rng_key: PRNG key for randomness\n",
    "    \n",
    "    Returns:\n",
    "        next token index\n",
    "    \"\"\"\n",
    "    # Get model predictions\n",
    "    logits = state.apply_fn({'params': state.params}, input_ids, training=False)\n",
    "    \n",
    "    # Take logits for the last position\n",
    "    next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
    "    \n",
    "    # Apply sampling strategy\n",
    "    if use_nucleus:\n",
    "        next_token = nucleus_sampling(next_token_logits, p=nucleus_p, temperature=temperature, rng_key=rng_key)\n",
    "    else:\n",
    "        next_token = top_k_sampling(next_token_logits, k=top_k, temperature=temperature, rng_key=rng_key)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "def generate_text(state, prompt, tokenizer, max_length=config.max_seq_len, temperature=0.7, top_k=500, \n",
    "                 use_nucleus=True, nucleus_p=0.95, seed=42, stop_at_eos=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model with advanced sampling.\n",
    "    \n",
    "    Args:\n",
    "        state: training state with model parameters\n",
    "        prompt: input text string to start generation\n",
    "        tokenizer: tokenizer for encoding/decoding\n",
    "        max_length: maximum number of tokens to generate\n",
    "        temperature: sampling temperature (0.1 = conservative, 1.0 = balanced, 2.0 = creative)\n",
    "        top_k: number of top candidates for top-k sampling (lower = more focused)\n",
    "        use_nucleus: whether to use nucleus (top-p) sampling instead of top-k\n",
    "        nucleus_p: probability threshold for nucleus sampling (0.9 = balanced)\n",
    "        seed: random seed for reproducibility\n",
    "        stop_at_eos: whether to stop generation at EOS token\n",
    "        verbose: whether to print generation progress\n",
    "    \n",
    "    Returns:\n",
    "        generated text string\n",
    "    \"\"\"\n",
    "    # Initialize random key\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='np')\n",
    "    input_ids = jnp.array(input_ids).reshape(1, -1)  # [1, seq_len]\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Generating text with:\")\n",
    "        print(f\"   Temperature: {temperature}\")\n",
    "        print(f\"   {'Nucleus (top-p)' if use_nucleus else 'Top-k'}: {nucleus_p if use_nucleus else top_k}\")\n",
    "        print(f\"   Max length: {max_length}\")\n",
    "        print(f\"   Prompt: '{prompt}'\")\n",
    "        print(\"    Generation:\")\n",
    "        print(prompt, end=\"\")\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # Split the random key for this step\n",
    "        rng_key, step_key = jax.random.split(rng_key)\n",
    "        \n",
    "        # Generate next token\n",
    "        next_token = generate_next_token(\n",
    "            state, input_ids, \n",
    "            temperature=temperature, \n",
    "            top_k=top_k,\n",
    "            use_nucleus=use_nucleus,\n",
    "            nucleus_p=nucleus_p,\n",
    "            rng_key=step_key\n",
    "        )\n",
    "        \n",
    "        # Convert to Python int\n",
    "        next_token = int(next_token)\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Check for EOS token\n",
    "        if stop_at_eos and next_token == tokenizer.eos_token_id:\n",
    "            if verbose:\n",
    "                print(\" [EOS]\")\n",
    "            break\n",
    "        \n",
    "        # Decode and print the new token if verbose\n",
    "        if verbose:\n",
    "            token_text = tokenizer.decode([next_token])\n",
    "            print(token_text, end=\"\", flush=True)\n",
    "        \n",
    "        # Update input_ids for next iteration\n",
    "        next_token_array = jnp.array([[next_token]])\n",
    "        input_ids = jnp.concatenate([input_ids, next_token_array], axis=1)\n",
    "        \n",
    "        # Truncate if sequence gets too long (to fit model's max_seq_len)\n",
    "        if input_ids.shape[1] > config.max_seq_len:\n",
    "            input_ids = input_ids[:, -config.max_seq_len:]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n Generation complete!\")\n",
    "    \n",
    "    # Decode the full generated text\n",
    "    full_text = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "66c9c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and collate function\n",
    "def collate(batch):\n",
    "    \"\"\"Collate function for DataLoader to handle TinyStories data.\"\"\"\n",
    "    # Extract text from batch\n",
    "    texts = [item['text'] for item in batch]\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        max_length=config.max_seq_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    return encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ae229e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7454,    11,   612, ..., 50256, 50256, 50256],\n",
       "       [13787,   318,   257, ..., 50256, 50256, 50256],\n",
       "       [ 7454,  2402,   257, ..., 50256, 50256, 50256],\n",
       "       ...,\n",
       "       [ 7454,   612,   373, ..., 50256, 50256, 50256],\n",
       "       [ 7454,  2402,   257, ..., 50256, 50256, 50256],\n",
       "       [ 7454,  2402,   257, ..., 50256, 50256, 50256]], shape=(64, 1024))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    # num_workers=int(os.cpu_count() / 2)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate,\n",
    "    # num_workers=int(os.cpu_count() / 2)\n",
    ")\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_file(text, step):\n",
    "    \n",
    "    dir = './generated_texts'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "    with open('generated_texts/{step}.txt', 'w') as f:\n",
    "        f.writelines(text + \"\\n\\n\")\n",
    "\n",
    "def train(resume_from_checkpoint=False, checkpoint_dir=\"./checkpoints\", save_every=1000):\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"smoljax-gpt\",\n",
    "        config={\n",
    "            \"vocab_size\": config.vocab_size,\n",
    "            \"max_seq_len\": config.max_seq_len,\n",
    "            \"d_model\": config.d_model,\n",
    "            \"num_layers\": config.num_layers,\n",
    "            \"num_heads\": config.num_heads,\n",
    "            \"d_ff\": config.d_ff,\n",
    "            \"dropout_rate\": config.dropout_rate,\n",
    "            \"learning_rate\": config.lr,\n",
    "            \"warmup_steps\": config.warmup_steps,\n",
    "            \"total_steps\": config.total_steps,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"num_epochs\": config.num_epochs,\n",
    "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "            \"save_every\": save_every,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        # num_workers=int(os.cpu_count() / 2)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=collate,\n",
    "        # num_workers=int(os.cpu_count() / 2)\n",
    "    )\n",
    "    \n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    train_state = create_train_state(rng, config)\n",
    "    \n",
    "    # Initialize variables\n",
    "    start_step = 0\n",
    "    tokens_processed = 0\n",
    "    \n",
    "    # Try to resume from checkpoint if requested\n",
    "    if resume_from_checkpoint:\n",
    "        print(\"Checking for existing checkpoints...\")\n",
    "        available_checkpoints = get_checkpoint_info(checkpoint_dir)\n",
    "        \n",
    "        if available_checkpoints:\n",
    "            print(f\"Found checkpoints at steps: {available_checkpoints}\")\n",
    "            restored_state, start_step = load_checkpoint(checkpoint_dir, train_state)\n",
    "            \n",
    "            if restored_state is not None:\n",
    "                train_state = restored_state\n",
    "                print(f\"Resuming training from step {start_step}\")\n",
    "                \n",
    "                # Estimate tokens processed (rough approximation)\n",
    "                tokens_processed = start_step * config.batch_size * config.max_seq_len\n",
    "                print(f\"Estimated tokens processed so far: {tokens_processed:,}\")\n",
    "            else:\n",
    "                print(\"Failed to load checkpoint, starting from scratch\")\n",
    "        else:\n",
    "            print(\"No checkpoints found, starting fresh training\")\n",
    "    \n",
    "    # Log model summary to wandb\n",
    "    total_params = sum([param.size for param in jax.tree_util.tree_leaves(train_state.params)])\n",
    "    wandb.log({\n",
    "        \"model/total_parameters\": total_params,\n",
    "        \"model/model_size_mb\": total_params * 4 / (1024**2),  # float32 = 4 bytes\n",
    "        \"checkpoint/resume_from\": start_step\n",
    "    })\n",
    "    \n",
    "    # Log detailed training summary\n",
    "    log_training_summary(train_state, config, total_params, tokens_processed)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = config.num_epochs\n",
    "    state = train_state.replace(step=start_step)  # Set correct step for LR schedule\n",
    "    global_step = start_step\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_losses = []\n",
    "        train_grad_norms = []\n",
    "        \n",
    "        # Process in batches with progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}, Step {step + 1}/{config.total_steps} [Train]\")\n",
    "        for batch in pbar:\n",
    "            # Convert to JAX array\n",
    "            batch = jnp.array(batch)\n",
    "            \n",
    "            # Count tokens (excluding padding)\n",
    "            batch_tokens = jnp.sum(batch != tokenizer.pad_token_id)\n",
    "            tokens_processed += int(batch_tokens)\n",
    "            \n",
    "            state, loss, grad_norm = train_step_accum(state, batch)\n",
    "            train_losses.append(float(loss))\n",
    "            train_grad_norms.append(float(grad_norm))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Get current learning rate\n",
    "            current_lr = float(state.opt_state[1].hyperparams['learning_rate'])\n",
    "            \n",
    "            # Log training metrics to wandb\n",
    "            wandb.log({\n",
    "                \"train/loss\": float(loss),\n",
    "                \"train/grad_norm\": float(grad_norm),\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                \"train/tokens_processed\": tokens_processed,\n",
    "                \"train/epoch\": epoch + 1,\n",
    "                \"train/batch_size\": config.batch_size,\n",
    "                \"train/step\": global_step\n",
    "            }, step=global_step)\n",
    "            \n",
    "            # Save checkpoint every save_every steps\n",
    "            if global_step % save_every == 0:\n",
    "                save_checkpoint(state, global_step, checkpoint_dir)\n",
    "                \n",
    "                # Also save model-only version for inference\n",
    "                save_model_for_inference(\n",
    "                    state, \n",
    "                    model_dir=\"./saved_models\", \n",
    "                    model_name=f\"smoljax_gpt_step_{global_step}\"\n",
    "                )\n",
    "                \n",
    "                # Log checkpoint info to wandb\n",
    "                wandb.log({\n",
    "                    \"checkpoint/last_saved_step\": global_step,\n",
    "                    \"checkpoint/tokens_at_save\": tokens_processed\n",
    "                }, step=global_step)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss:.4f}\", \n",
    "                \"grad_norm\": f\"{grad_norm:.4f}\",\n",
    "                \"lr\": f\"{current_lr:.6f}\",\n",
    "                \"tokens\": f\"{tokens_processed:,}\",\n",
    "                \"step\": global_step\n",
    "            })\n",
    "        \n",
    "        # Validation\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        \n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        for batch in pbar:\n",
    "            batch = jnp.array(batch)\n",
    "            loss, acc = eval_step(state, batch)\n",
    "            val_losses.append(float(loss))\n",
    "            # val_accs.append(float(acc))\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss:.4f}\", \n",
    "                \"acc\": f\"{acc:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_train_grad_norm = np.mean(train_grad_norms)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        # avg_val_acc = np.mean(val_accs)\n",
    "        \n",
    "        # Log epoch metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch/train_loss\": avg_train_loss,\n",
    "            \"epoch/val_loss\": avg_val_loss,\n",
    "            # \"epoch/val_accuracy\": avg_val_acc,\n",
    "            \"epoch/train_grad_norm\": avg_train_grad_norm,\n",
    "            \"epoch/epoch\": epoch + 1\n",
    "        }, step=global_step)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Grad Norm: {avg_train_grad_norm:.4f}\")\n",
    "        # print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
    "        print(f\"  Tokens Processed: {tokens_processed:,}\")\n",
    "        print(f\"  Global Step: {global_step}\")\n",
    "        \n",
    "        # Save checkpoint at end of each epoch\n",
    "        save_checkpoint(state, global_step, checkpoint_dir)\n",
    "        \n",
    "    # Save final checkpoint\n",
    "    print(\"\\n Saving final checkpoint...\")\n",
    "    save_checkpoint(state, global_step, checkpoint_dir)\n",
    "    \n",
    "    # Generate some text\n",
    "    print(\"\\nGenerating text...\")\n",
    "    generated = generate_text(state, \"The future of artificial intelligence\", tokenizer, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    save_to_file(generated, step)\n",
    "    \n",
    "    # Log final generation to wandb\n",
    "    wandb.log({\n",
    "        \"generation/sample_text\": generated,\n",
    "        \"generation/prompt\": \"The future of artificial intelligence\",\n",
    "        \"training/final_step\": global_step,\n",
    "        \"training/final_tokens\": tokens_processed\n",
    "    })\n",
    "    \n",
    "    # Log final training summary\n",
    "    log_training_summary(state, config, total_params, tokens_processed)\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "22f407e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>checkpoint/resume_from</td><td>▁</td></tr><tr><td>model/model_size_mb</td><td>▁</td></tr><tr><td>model/total_parameters</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>checkpoint/resume_from</td><td>0</td></tr><tr><td>model/model_size_mb</td><td>946.01789</td></tr><tr><td>model/total_parameters</td><td>247992913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-thunder-1</strong> at: <a href='https://wandb.ai/rentio/smoljax-gpt/runs/381ithva' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt/runs/381ithva</a><br> View project at: <a href='https://wandb.ai/rentio/smoljax-gpt' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250923_131907-381ithva/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yuvrajsingh9886/smoljax/GPT/wandb/run-20250923_133733-01f5jipm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rentio/smoljax-gpt/runs/01f5jipm' target=\"_blank\">fearless-wildflower-2</a></strong> to <a href='https://wandb.ai/rentio/smoljax-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rentio/smoljax-gpt' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rentio/smoljax-gpt/runs/01f5jipm' target=\"_blank\">https://wandb.ai/rentio/smoljax-gpt/runs/01f5jipm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train]:   0%|          | 0/33121 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    exitcode = _main(fd, parent_sentinel)AttributeError: \n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    exitcode = _main(fd, parent_sentinel)AttributeError: \n",
      "  File \"/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'collate_fn' on <module '__main__' (built-in)>\n",
      "Epoch 1/1 [Train]:   0%|          | 0/33121 [00:04<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 95542, 95543, 95544, 95545) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1285\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 95543) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[236], line 90\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(resume_from_checkpoint, checkpoint_dir, save_every)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Process in batches with progress bar\u001b[39;00m\n\u001b[1;32m     89\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Convert to JAX array\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     batch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Count tokens (excluding padding)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1492\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1492\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1454\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1454\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1456\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1298\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1297\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1300\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 95542, 95543, 95544, 95545) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e6d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
