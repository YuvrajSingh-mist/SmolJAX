{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ICLM60p0rP9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ICLM60p0rP9",
        "outputId": "e67bff44-635c-45c2-c43c-7b399fd927bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.38.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]) (1.16.2)\n",
            "Requirement already satisfied: jax-cuda12-plugin<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: jax-cuda12-pjrt==0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin<=0.5.3,>=0.5.3->jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (0.5.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (12.6.80)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-nvcc-cu12\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "Successfully installed nvidia-cuda-nvcc-cu12-12.9.86\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jax>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.5.3)\n",
            "Requirement already satisfied: jaxlib>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax) (2.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.3->optax) (1.16.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.10.6)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax) (2.0.2)\n",
            "Requirement already satisfied: jax>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from flax) (0.5.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax) (0.11.24)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (1.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.23.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install tqdm\n",
        "!pip install jax[cuda12]\n",
        "!pip install optax\n",
        "!pip install flax\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "151fd62f",
      "metadata": {
        "id": "151fd62f"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax.jax_utils import prefetch_to_device\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.training import checkpoints\n",
        "from flax import struct\n",
        "import optax\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Any\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZdBPFYAKiv0F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdBPFYAKiv0F",
        "outputId": "fb3021fd-6523-4ed7-857d-ecc3ed2cc4bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Information:\n",
            "JAX version: 0.5.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3071427205.py:33: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  print(f\"Platform: {jax.lib.xla_bridge.get_backend().platform}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: gpu\n",
            "Device count: 1\n",
            "‚ùå Error checking TPU: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory. Available backends are ['cpu', 'cuda']\n",
            "\n",
            "Test computation result: 55\n",
            "Computation device: cuda:0\n",
            "jax.devices(): [CudaDevice(id=0)]\n",
            "jax.device_count(): 1\n",
            "jax.local_device_count(): 1\n",
            "jax.process_count(): 1\n",
            "jax.process_index(): 0\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "# Check TPU availability and setup\n",
        "def check_tpu_setup():\n",
        "    \"\"\"Check if TPU is available and properly configured.\"\"\"\n",
        "    try:\n",
        "        # Check if we're running on TPU\n",
        "        if jax.devices('tpu'):\n",
        "            print(\"‚úÖ TPU detected!\")\n",
        "            devices = jax.devices('tpu')\n",
        "            print(f\"   TPU devices: {len(devices)}\")\n",
        "            for i, device in enumerate(devices):\n",
        "                print(f\"   Device {i}: {device}\")\n",
        "\n",
        "            # Print TPU-specific info\n",
        "            print(f\"   JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
        "            print(f\"   Total TPU cores: {jax.device_count()}\")\n",
        "            print(f\"   Local devices: {jax.local_device_count()}\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå No TPU devices found\")\n",
        "            print(f\"   Available devices: {jax.devices()}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error checking TPU: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check current setup\n",
        "print(\"Device Information:\")\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Platform: {jax.lib.xla_bridge.get_backend().platform}\")\n",
        "print(f\"Device count: {jax.device_count()}\")\n",
        "\n",
        "# Check TPU specifically\n",
        "has_tpu = check_tpu_setup()\n",
        "\n",
        "# Test a simple computation\n",
        "test_array = jnp.array([1, 2, 3, 4, 5])\n",
        "result = jnp.sum(test_array ** 2)\n",
        "print(f\"\\nTest computation result: {result}\")\n",
        "print(f\"Computation device: {result.device}\")\n",
        "\n",
        "print(\"jax.devices():\", jax.devices())\n",
        "print(\"jax.device_count():\", jax.device_count())\n",
        "print(\"jax.local_device_count():\", jax.local_device_count())\n",
        "print(\"jax.process_count():\", jax.process_count())\n",
        "print(\"jax.process_index():\", jax.process_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "PZzi9sSvC5C3",
      "metadata": {
        "id": "PZzi9sSvC5C3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['FLAX_USE_LEGACY_CHECKPOINTS'] = '1'  # Force legacy checkpoints to avoid Orbax issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "80NfOZiEDOyb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80NfOZiEDOyb",
        "outputId": "0f29afbb-4cf9-4784-b75b-a19b537af898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): object address  : 0x79fbf5db1660\n",
            "object refcount : 3\n",
            "object type     : 0xa274e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!hf auth login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cafc054c",
      "metadata": {
        "id": "cafc054c"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=\"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\")\n",
        "tokenizer.pad_token = '[PAD]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d012c592",
      "metadata": {
        "id": "d012c592"
      },
      "outputs": [],
      "source": [
        "# Configuration class for model parameters\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int = tokenizer.vocab_size + 47\n",
        "    max_seq_len: int = 256\n",
        "    d_model: int = 768\n",
        "    num_layers: int = 12\n",
        "    num_heads: int = 12\n",
        "    d_ff: int = 3072\n",
        "    dropout_rate: float = 0.1\n",
        "    lr: float = 6e-4\n",
        "    warmup_steps: int = 700\n",
        "    total_steps: int = 20000\n",
        "    batch_size: int = 256\n",
        "    required_bsz_tokens: int = 524288\n",
        "    gradient_accumulation_steps: int = int(required_bsz_tokens // (batch_size * max_seq_len))\n",
        "    mixed_precision: bool = True\n",
        "    num_epochs: int = 1\n",
        "    eval_steps: int = 200\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "geW4X-AIp68l",
      "metadata": {
        "id": "geW4X-AIp68l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac5dfca-f17b-492c-84d7-07ef5e6dfb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set JAX matmul precision to bfloat16 (mixed precision enabled)\n"
          ]
        }
      ],
      "source": [
        "from jax import config as jax_config\n",
        "\n",
        "# Set matmul precision based on mixed_precision setting\n",
        "if config.mixed_precision:\n",
        "    jax_config.update(\"jax_default_matmul_precision\", \"bfloat16\")\n",
        "    print(\"Set JAX matmul precision to bfloat16 (mixed precision enabled)\")\n",
        "else:\n",
        "    jax_config.update(\"jax_default_matmul_precision\", \"float32\")\n",
        "    print(\"Set JAX matmul precision to float32 (mixed precision disabled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "30a08a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30a08a28",
        "outputId": "4acbccf1-2a6e-4ed3-d708-9df7275b21f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dtype: <class 'jax.numpy.bfloat16'> (mixed_precision=True)\n"
          ]
        }
      ],
      "source": [
        "# Helper function to get the appropriate dtype based on mixed_precision setting\n",
        "def get_dtype():\n",
        "    \"\"\"Return bfloat16 if mixed_precision is True, else float32.\"\"\"\n",
        "    return jnp.bfloat16 if config.mixed_precision else jnp.float32\n",
        "\n",
        "print(f\"Using dtype: {get_dtype()} (mixed_precision={config.mixed_precision})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "jRhnpTa2jDJy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "jRhnpTa2jDJy",
        "outputId": "87176414-4789-474b-c5d1-181681a36af4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory. Available backends are ['cpu', 'cuda']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3594055056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Apply optimizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0moptimize_for_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3594055056.py\u001b[0m in \u001b[0;36moptimize_for_tpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize_for_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Apply TPU-specific optimizations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not running on TPU, skipping optimizations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mdevices\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDevice\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m   \"\"\"\n\u001b[0;32m-> 1115\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0mplatform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m ) -> xla_client.Client:\n\u001b[0;32m-> 1049\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_backend_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         raise RuntimeError(f\"Backend '{platform}' failed to initialize: \"\n\u001b[0m\u001b[1;32m   1035\u001b[0m                            \u001b[0;34mf\"{_backend_errors[platform]}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                            f'Available backends are {list(bs)}')\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory. Available backends are ['cpu', 'cuda']"
          ]
        }
      ],
      "source": [
        "def optimize_for_tpu():\n",
        "    \"\"\"Apply TPU-specific optimizations.\"\"\"\n",
        "    if not jax.devices('tpu'):\n",
        "        print(\"Not running on TPU, skipping optimizations\")\n",
        "        return\n",
        "\n",
        "    print(\"üöÄ Applying TPU optimizations...\")\n",
        "\n",
        "    # Set XLA flags for better TPU performance\n",
        "    import os\n",
        "    os.environ.setdefault('XLA_FLAGS',\n",
        "        '--xla_force_host_platform_device_count=1')\n",
        "\n",
        "    # # Enable mixed precision if not already enabled\n",
        "    if not config.mixed_precision:\n",
        "        print(\"   Enabling mixed precision for TPU\")\n",
        "        config.mixed_precision = True\n",
        "\n",
        "    print(\" TPU optimizations applied!\")\n",
        "\n",
        "# Apply optimizations\n",
        "optimize_for_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "N3D8_qec1GTW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3D8_qec1GTW",
        "outputId": "f681748c-67f1-46b4-bcc0-1269800c6f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "502276f0",
      "metadata": {
        "id": "502276f0"
      },
      "outputs": [],
      "source": [
        "from flax.training import train_state\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    grad_accum: Any = None\n",
        "    accum_step: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2fa95aed",
      "metadata": {
        "id": "2fa95aed"
      },
      "outputs": [],
      "source": [
        "#Loading TinyStories dataset from Huggingface\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", token='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')\n",
        "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "16494b05",
      "metadata": {
        "id": "16494b05"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.head_size = self.d_model // self.num_heads\n",
        "\n",
        "        # Proper initialization for attention layers\n",
        "        self.d_Q = nn.Dense(\n",
        "            features=self.head_size,\n",
        "            use_bias=False,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.d_K = nn.Dense(\n",
        "            features=self.head_size,\n",
        "            use_bias=False,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.d_V = nn.Dense(\n",
        "            features=self.head_size,\n",
        "            use_bias=False,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.d_O = nn.Dense(\n",
        "            features=self.d_model,\n",
        "            use_bias=False,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        B, T, C = x.shape\n",
        "        query = self.d_Q(x)\n",
        "        key = self.d_K(x)\n",
        "        value = self.d_V(x)\n",
        "\n",
        "        # Proper attention scaling using head_size\n",
        "        weights = jnp.matmul(query, key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
        "\n",
        "        # Better causal mask using -inf\n",
        "        mask = jnp.tril(jnp.ones((T, T)))\n",
        "        weights = jnp.where(mask == 0, -jnp.inf, weights)\n",
        "\n",
        "        weights = nn.softmax(weights, axis=-1)\n",
        "        # weights = self.dropout(weights, deterministic=not training)  # Apply dropout to attention weights\n",
        "\n",
        "        out = jnp.matmul(weights, value)\n",
        "        out = self.d_O(out)\n",
        "        out = self.dropout(out, deterministic=not training)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6eb96ba1",
      "metadata": {
        "id": "6eb96ba1"
      },
      "outputs": [],
      "source": [
        "class MHA(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.heads = [Attention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
        "\n",
        "        # Proper initialization for output projection\n",
        "        self.linear = nn.Dense(\n",
        "            features=self.d_model,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out, deterministic=not training)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1fd804b6",
      "metadata": {
        "id": "1fd804b6"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        # Proper initialization for MLP layers\n",
        "        self.fc1 = nn.Dense(\n",
        "            features=self.d_ff,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.fc2 = nn.Dense(\n",
        "            features=self.d_model,\n",
        "            dtype=get_dtype(),\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        x = self.fc1(x)\n",
        "        x = nn.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x, deterministic=not training)  # Remove duplicate GELU\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c1b2a846",
      "metadata": {
        "id": "c1b2a846"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "\n",
        "    def setup(self):\n",
        "        self.attention = MHA(self.d_model, self.num_heads, self.dropout_rate)\n",
        "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
        "        self.ln1 = nn.LayerNorm(dtype=get_dtype())\n",
        "        self.ln2 = nn.LayerNorm(dtype=get_dtype())\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        attn =  self.attention(self.ln1(x), training)\n",
        "        x = x + attn * (config.num_layers ** -0.5)\n",
        "        mlp_out = self.mlp(self.ln2(x), training)\n",
        "        x = x + mlp_out * (config.num_layers ** -0.5)\n",
        "        # x = x * (config.num_layers ** -0.5)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c83412e4",
      "metadata": {
        "id": "c83412e4"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    d_model: int = config.d_model\n",
        "    num_heads: int = config.num_heads\n",
        "    d_ff: int = config.d_ff\n",
        "    dropout_rate: float = config.dropout_rate\n",
        "    vocab_size: int = config.vocab_size\n",
        "    seq_len: int = config.max_seq_len\n",
        "\n",
        "    def setup(self):\n",
        "        self.embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model, dtype=get_dtype(), embedding_init=nn.initializers.normal(stddev=0.02))\n",
        "        self.positional_embedding = self.param(\n",
        "            \"positional_embeddings\",  # name\n",
        "            lambda key: jax.random.normal(key, (1, self.seq_len, self.d_model), dtype=get_dtype()) * 0.01\n",
        "        )\n",
        "        self.decoder = [TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(config.num_layers)]\n",
        "        self.linear_out = nn.Dense(features=self.vocab_size, dtype=get_dtype(), kernel_init=nn.initializers.normal(stddev=0.02), bias_init=nn.initializers.zeros)  # Zero bias initialization\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        B,T = x.shape\n",
        "        embeds = self.embedding_table(x)  # (B,T,d_model)\n",
        "        C = embeds.shape[-1]\n",
        "        pos_embeds = self.positional_embedding[:, :T, :]  # (1,T,d_model)\n",
        "        x = embeds + pos_embeds  # (B,T,d_model)\n",
        "        # pad_mask = (x != tokenizer.pad_token_id).astype(get_dtype())\n",
        "        # x = x * pad_mask\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x, training=training)\n",
        "\n",
        "        x = self.linear_out(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6eb78ff",
      "metadata": {
        "id": "b6eb78ff"
      },
      "outputs": [],
      "source": [
        "# Add this cell to inspect the model summary like torchsummary\n",
        "\n",
        "from flax.linen import tabulate\n",
        "import jax\n",
        "\n",
        "# Initialize model\n",
        "model = GPT()\n",
        "key = jax.random.PRNGKey(0)\n",
        "x = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # Dummy input for tabulation\n",
        "\n",
        "# Tabulate the model structure\n",
        "tabulate_fn = tabulate(model, key, console_kwargs={'width': 120})\n",
        "\n",
        "# Count total parameters\n",
        "params = model.init(key, x)['params']\n",
        "total_params = sum(jax.tree_util.tree_leaves(jax.tree.map(lambda arr: arr.size, params)))\n",
        "\n",
        "# Get raw summary and clean ANSI codes\n",
        "raw_summary = tabulate_fn(x, training=True)\n",
        "# Remove ANSI color codes for clean logging\n",
        "clean_summary = re.sub(r'\\x1b\\[[0-9;]*m', '', raw_summary)\n",
        "\n",
        "# Save to log file with clean formatting\n",
        "with open('model_summary.txt', 'w') as f:\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(\"GPT MODEL ARCHITECTURE SUMMARY\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
        "    f.write(f\"Model Configuration:\\n\")\n",
        "    f.write(f\"  - Vocabulary Size: {config.vocab_size:,}\\n\")\n",
        "    f.write(f\"  - Max Sequence Length: {config.max_seq_len}\\n\")\n",
        "    f.write(f\"  - Model Dimension: {config.d_model}\\n\")\n",
        "    f.write(f\"  - Number of Layers: {config.num_layers}\\n\")\n",
        "    f.write(f\"  - Number of Heads: {config.num_heads}\\n\")\n",
        "    f.write(f\"  - Feed Forward Dimension: {config.d_ff}\\n\")\n",
        "    f.write(f\"  - Dropout Rate: {config.dropout_rate}\\n\\n\")\n",
        "    f.write(\"Detailed Layer Information:\\n\")\n",
        "    f.write(\"-\" * 40 + \"\\n\")\n",
        "    f.write(clean_summary)\n",
        "\n",
        "print(f\"Model summary saved to model_summary.txt\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 2 / (1024**2):.1f} MB (bfloat16)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b39759",
      "metadata": {
        "id": "57b39759"
      },
      "outputs": [],
      "source": [
        "def create_learning_rate_schedule():\n",
        "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
        "    config = GPTConfig()\n",
        "    def schedule(step):\n",
        "        # Linear warmup\n",
        "        warmup_ratio = jnp.minimum(1.0, step / config.warmup_steps)\n",
        "        # Cosine decay after warmup\n",
        "        decay_ratio = jnp.maximum(0.0, (step - config.warmup_steps) / (config.total_steps - config.warmup_steps))\n",
        "        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * decay_ratio))\n",
        "        return config.lr * warmup_ratio * cosine_decay\n",
        "\n",
        "    return schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687a1842",
      "metadata": {
        "id": "687a1842"
      },
      "outputs": [],
      "source": [
        "def compute_ce_loss(logits, labels):\n",
        "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
        "    labels = labels[:, 1:]\n",
        "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
        "\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    pad_mask = (labels != tokenizer.pad_token_id)\n",
        "    loss = jnp.where(pad_mask, loss, 0.0)\n",
        "    return loss.sum () / pad_mask.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23fd1398",
      "metadata": {
        "id": "23fd1398"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, config):\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "    model = GPT()\n",
        "\n",
        "    # Initialize parameters\n",
        "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
        "    params = model.init(rng, dummy_input)['params']\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    lr_schedule = create_learning_rate_schedule()\n",
        "\n",
        "    # Create optimizer with stronger gradient clipping and better settings\n",
        "    tx = optax.chain(\n",
        "        optax.clip_by_global_norm(1.0),  # Much stronger clipping (was 1.0)\n",
        "        optax.adamw(\n",
        "            learning_rate=lr_schedule,\n",
        "            b1=0.9,\n",
        "            b2=0.95,\n",
        "            weight_decay=0.01,  # Reduced weight decay (was 0.1)\n",
        "            eps=1e-9  # Added epsilon for numerical stability\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=tx\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79af46bc",
      "metadata": {
        "id": "79af46bc"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch, step):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
        "        loss = compute_ce_loss(logits, batch)\n",
        "        return loss, logits\n",
        "\n",
        "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "\n",
        "    # Compute gradient norm for logging\n",
        "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
        "\n",
        "    # Update the parameters\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss, grad_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c7ffe7",
      "metadata": {
        "id": "35c7ffe7"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step_accum(state, batch, step):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
        "        loss = compute_ce_loss(logits, batch)\n",
        "        return loss, logits\n",
        "\n",
        "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "    # Compute gradient norm for logging\n",
        "\n",
        "    if state.grad_accum is None:\n",
        "\n",
        "        state = state.replace(\n",
        "            grad_accum=jax.tree_util.tree_map(jnp.zeros_like, grads),\n",
        "            accum_step=0,\n",
        "        )\n",
        "    new_accum = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, state.grad_accum, grads)\n",
        "    new_step = state.accum_step + 1\n",
        "\n",
        "    # print(new_accum)\n",
        "    # print(new_step)\n",
        "\n",
        "    def apply_update(_):\n",
        "        mean_grads = jax.tree_util.tree_map(lambda g: g / config.gradient_accumulation_steps, new_accum)\n",
        "        # print(\"Grads: \", mean_grads)\n",
        "        grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(mean_grads)]))\n",
        "        # print(\"Here: \", jnp.array(grad_norm, float))\n",
        "        new_state = state.apply_gradients(grads = mean_grads)\n",
        "        new_grad_accum = jax.tree_util.tree_map(jnp.zeros_like, new_accum)\n",
        "\n",
        "        return new_state.replace(grad_accum=new_grad_accum, accum_step=0), grad_norm\n",
        "\n",
        "    def carry_forward(_):\n",
        "\n",
        "        # dummy_grads = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "\n",
        "        return state.replace(grad_accum=new_accum, accum_step=new_step), -1.0\n",
        "\n",
        "\n",
        "    state, grad_norm = jax.lax.cond(new_step == config.gradient_accumulation_steps, apply_update, carry_forward, operand=None)\n",
        "\n",
        "    return state, loss, grad_norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95fc0cc",
      "metadata": {
        "id": "f95fc0cc"
      },
      "outputs": [],
      "source": [
        "# JIT-compiled evaluation step\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    \"\"\"Single evaluation step.\"\"\"\n",
        "    logits = state.apply_fn({'params': state.params}, batch, training=False)\n",
        "    loss = compute_ce_loss(logits, batch)\n",
        "\n",
        "    return loss, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a9d0a7",
      "metadata": {
        "id": "12a9d0a7"
      },
      "outputs": [],
      "source": [
        "# Vectorized prediction function using vmap\n",
        "@jax.jit\n",
        "def predict_batch(state, batch):\n",
        "    \"\"\"Generate predictions for a batch using vmap.\"\"\"\n",
        "    return state.apply_fn({'params': state.params}, batch, training=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c34940",
      "metadata": {
        "id": "63c34940"
      },
      "outputs": [],
      "source": [
        "# Helper function to create wandb summary table\n",
        "def log_training_summary(state, config, total_params, tokens_processed):\n",
        "    \"\"\"Log a comprehensive training summary to wandb.\"\"\"\n",
        "\n",
        "    # Create a summary table (ensure all values are strings for wandb compatibility)\n",
        "    summary_data = [\n",
        "        [\"Model\", \"SmolJAX GPT\"],\n",
        "        [\"Total Parameters\", f\"{total_params:,}\"],\n",
        "        [\"Model Size (MB)\", f\"{total_params * 2 / (1024**2):.1f}\"],\n",
        "        [\"Vocabulary Size\", f\"{config.vocab_size:,}\"],\n",
        "        [\"Max Sequence Length\", f\"{config.max_seq_len}\"],\n",
        "        [\"Model Dimension\", f\"{config.d_model}\"],\n",
        "        [\"Number of Layers\", f\"{config.num_layers}\"],\n",
        "        [\"Number of Heads\", f\"{config.num_heads}\"],\n",
        "        [\"Feed Forward Dimension\", f\"{config.d_ff}\"],\n",
        "        [\"Dropout Rate\", f\"{config.dropout_rate}\"],\n",
        "        [\"Learning Rate\", f\"{config.lr}\"],\n",
        "        [\"Batch Size\", f\"{config.batch_size}\"],\n",
        "        [\"Total Epochs\", f\"{config.num_epochs}\"],\n",
        "        [\"Tokens Processed\", f\"{tokens_processed:,}\"],\n",
        "        [\"Training Step\", f\"{int(state.step)}\"]\n",
        "    ]\n",
        "\n",
        "    # Create wandb table\n",
        "    table = wandb.Table(\n",
        "        columns=[\"Metric\", \"Value\"],\n",
        "        data=summary_data\n",
        "    )\n",
        "\n",
        "    wandb.log({\"training_summary\": table})\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a98639",
      "metadata": {
        "id": "e4a98639"
      },
      "outputs": [],
      "source": [
        "# Checkpoint management functions\n",
        "def save_checkpoint(state, step, checkpoint_dir=\"./checkpoints\", keep=5):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    checkpoints.save_checkpoint(\n",
        "        ckpt_dir=checkpoint_dir,\n",
        "        target=state,\n",
        "        step=step,\n",
        "        keep=keep,  # Keep only the last 5 checkpoints\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "    print(f\"Checkpoint saved at step {step} in {checkpoint_dir}\")\n",
        "\n",
        "    # Log to wandb if available\n",
        "    if wandb.run is not None:\n",
        "        wandb.log({\n",
        "            \"checkpoint/step\": step,\n",
        "            \"checkpoint/saved\": 1\n",
        "        }, step=step)\n",
        "\n",
        "def load_checkpoint(checkpoint_dir=\"./checkpoints\", state=None):\n",
        "    \"\"\"Load the latest checkpoint.\"\"\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
        "        return None, 0\n",
        "\n",
        "    # Check if there are any checkpoints\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
        "    if not checkpoint_files:\n",
        "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
        "        return None, 0\n",
        "\n",
        "    try:\n",
        "        # Load the latest checkpoint\n",
        "        restored_state = checkpoints.restore_checkpoint(\n",
        "            ckpt_dir=checkpoint_dir,\n",
        "            target=state\n",
        "        )\n",
        "\n",
        "        # Get the step number from the checkpoint\n",
        "        latest_step = checkpoints.latest_checkpoint(checkpoint_dir)\n",
        "        if latest_step:\n",
        "            step = int(latest_step.split('_')[-1])\n",
        "            print(f\"Checkpoint loaded from step {step}\")\n",
        "            return restored_state, step\n",
        "        else:\n",
        "            print(f\"Could not determine step from checkpoint\")\n",
        "            return None, 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "def get_checkpoint_info(checkpoint_dir=\"./checkpoints\"):\n",
        "    \"\"\"Get information about available checkpoints.\"\"\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        return []\n",
        "\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
        "    checkpoint_steps = []\n",
        "\n",
        "    for file in checkpoint_files:\n",
        "        try:\n",
        "            step = int(file.split('_')[-1])\n",
        "            checkpoint_steps.append(step)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return sorted(checkpoint_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522e41b1",
      "metadata": {
        "id": "522e41b1"
      },
      "outputs": [],
      "source": [
        "# Model-only saving/loading functions for inference\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "def save_model_for_inference(state, model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Save only model parameters and config for inference (much smaller files).\"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save just the parameters (no optimizer state)\n",
        "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
        "    with open(params_path, 'wb') as f:\n",
        "        pickle.dump(state.params, f)\n",
        "\n",
        "    # Save model configuration\n",
        "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
        "    config_dict = {\n",
        "        \"vocab_size\": config.vocab_size,\n",
        "        \"max_seq_len\": config.max_seq_len,\n",
        "        \"d_model\": config.d_model,\n",
        "        \"num_layers\": config.num_layers,\n",
        "        \"num_heads\": config.num_heads,\n",
        "        \"d_ff\": config.d_ff,\n",
        "        \"dropout_rate\": config.dropout_rate\n",
        "    }\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "\n",
        "    # Save tokenizer info\n",
        "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
        "    tokenizer_info = {\n",
        "        \"tokenizer_name\": \"gpt2\",\n",
        "        \"vocab_size\": len(tokenizer),\n",
        "        \"pad_token\": tokenizer.pad_token,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token\": tokenizer.eos_token,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "    with open(tokenizer_path, 'w') as f:\n",
        "        json.dump(tokenizer_info, f, indent=2)\n",
        "\n",
        "    print(f\"   Model saved for inference:\")\n",
        "    print(f\"   Parameters: {params_path}\")\n",
        "    print(f\"   Config: {config_path}\")\n",
        "    print(f\"   Tokenizer info: {tokenizer_path}\")\n",
        "\n",
        "    # Calculate file sizes\n",
        "    params_size = os.path.getsize(params_path) / (1024**2)  # MB\n",
        "    print(f\"   Model size: {params_size:.1f} MB\")\n",
        "\n",
        "    return params_path, config_path, tokenizer_path\n",
        "\n",
        "def load_model_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Load model parameters and config for inference.\"\"\"\n",
        "\n",
        "    # Load configuration\n",
        "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "\n",
        "    # Recreate config object\n",
        "    inference_config = GPTConfig(**config_dict)\n",
        "\n",
        "    # Load parameters\n",
        "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
        "    if not os.path.exists(params_path):\n",
        "        raise FileNotFoundError(f\"Parameters file not found: {params_path}\")\n",
        "\n",
        "    with open(params_path, 'rb') as f:\n",
        "        params = pickle.load(f)\n",
        "\n",
        "    # Load tokenizer info\n",
        "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
        "    tokenizer_info = None\n",
        "    if os.path.exists(tokenizer_path):\n",
        "        with open(tokenizer_path, 'r') as f:\n",
        "            tokenizer_info = json.load(f)\n",
        "\n",
        "    print(f\" Model loaded for inference:\")\n",
        "    print(f\"   Config: {config_dict}\")\n",
        "    if tokenizer_info:\n",
        "        print(f\"   Tokenizer: {tokenizer_info['tokenizer_name']}\")\n",
        "\n",
        "    return params, inference_config, tokenizer_info\n",
        "\n",
        "def create_inference_model(params, inference_config):\n",
        "    \"\"\"Create a model instance for inference (no training state).\"\"\"\n",
        "\n",
        "    # Create model with loaded config\n",
        "    model = GPT(\n",
        "        d_model=inference_config.d_model,\n",
        "        num_heads=inference_config.num_heads,\n",
        "        d_ff=inference_config.d_ff,\n",
        "        dropout_rate=inference_config.dropout_rate,\n",
        "        vocab_size=inference_config.vocab_size,\n",
        "        seq_len=inference_config.max_seq_len\n",
        "    )\n",
        "\n",
        "    # Create apply function with loaded parameters\n",
        "    def inference_apply(inputs, training=False):\n",
        "        return model.apply({'params': params}, inputs, training=training)\n",
        "\n",
        "    return model, inference_apply\n",
        "\n",
        "def load_and_setup_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
        "    \"\"\"Complete setup for inference - load everything and return ready-to-use functions.\"\"\"\n",
        "\n",
        "    # Load model components\n",
        "    params, inference_config, tokenizer_info = load_model_for_inference(model_dir, model_name)\n",
        "\n",
        "    # Create model and inference function\n",
        "    model, inference_apply = create_inference_model(params, inference_config)\n",
        "\n",
        "    # Setup tokenizer (you might want to load this separately)\n",
        "    if tokenizer_info and tokenizer_info.get('tokenizer_name') == 'gpt2':\n",
        "        from transformers import AutoTokenizer\n",
        "        inference_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        inference_tokenizer.pad_token = tokenizer_info.get('pad_token', '[PAD]')\n",
        "        print(f\" Tokenizer setup complete\")\n",
        "    else:\n",
        "        inference_tokenizer = None\n",
        "        print(\" No tokenizer info found, you'll need to setup tokenizer manually\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'apply_fn': inference_apply,\n",
        "        'params': params,\n",
        "        'config': inference_config,\n",
        "        'tokenizer': inference_tokenizer\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6200ac8",
      "metadata": {
        "id": "e6200ac8"
      },
      "outputs": [],
      "source": [
        "# Advanced Text Generation with Top-K and Temperature Sampling\n",
        "import jax.random as random\n",
        "\n",
        "def top_k_sampling(logits, k=550, temperature=0.7, rng_key=None):\n",
        "    \"\"\"\n",
        "    Apply top-k sampling with temperature to logits.\n",
        "\n",
        "    Args:\n",
        "        logits: [vocab_size] array of logits\n",
        "        k: number of top candidates to keep\n",
        "        temperature: sampling temperature (lower = more deterministic)\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        sampled token index\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Get top-k indices and values\n",
        "    top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n",
        "\n",
        "    # Convert to probabilities\n",
        "    top_k_probs = jax.nn.softmax(top_k_logits)\n",
        "\n",
        "    # Sample from the top-k distribution\n",
        "    sampled_idx = jax.random.categorical(rng_key, jnp.log(top_k_probs))\n",
        "\n",
        "    # Return the actual token index\n",
        "    return top_k_indices[sampled_idx]\n",
        "\n",
        "def nucleus_sampling(logits, p=0.95, temperature=0.7, rng_key=None):\n",
        "    \"\"\"\n",
        "    Apply nucleus (top-p) sampling with temperature to logits.\n",
        "\n",
        "    Args:\n",
        "        logits: [vocab_size] array of logits\n",
        "        p: cumulative probability threshold\n",
        "        temperature: sampling temperature\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        sampled token index\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    # logits = logits / temperature\n",
        "\n",
        "    # Convert to probabilities and sort\n",
        "    probs = jax.nn.softmax(logits)\n",
        "    sorted_indices = jnp.argsort(probs)[::-1]  # Sort in descending order\n",
        "    sorted_probs = probs[sorted_indices]\n",
        "\n",
        "    # Find cumulative probabilities\n",
        "    cumsum_probs = jnp.cumsum(sorted_probs)\n",
        "\n",
        "    # Find the cutoff index where cumsum exceeds p\n",
        "    cutoff = jnp.searchsorted(cumsum_probs, p)\n",
        "    cutoff = jnp.maximum(cutoff, 1)  # Keep at least one token\n",
        "\n",
        "    # Keep only tokens within the nucleus\n",
        "    nucleus_indices = sorted_indices[:cutoff]\n",
        "    nucleus_probs = sorted_probs[:cutoff]\n",
        "    nucleus_probs = nucleus_probs / jnp.sum(nucleus_probs)  # Renormalize\n",
        "\n",
        "    # Sample from the nucleus\n",
        "    sampled_idx = jax.random.categorical(rng_key, jnp.log(nucleus_probs))\n",
        "\n",
        "    return nucleus_indices[sampled_idx]\n",
        "\n",
        "@jax.jit\n",
        "def generate_next_token(state, input_ids, temperature=1.0, top_k=50, use_nucleus=False, nucleus_p=0.9, rng_key=None):\n",
        "    \"\"\"\n",
        "    Generate the next token using the model.\n",
        "\n",
        "    Args:\n",
        "        state: training state with model parameters\n",
        "        input_ids: current sequence [batch_size, seq_len]\n",
        "        temperature: sampling temperature\n",
        "        top_k: number of top candidates for top-k sampling\n",
        "        use_nucleus: whether to use nucleus sampling instead of top-k\n",
        "        nucleus_p: probability threshold for nucleus sampling\n",
        "        rng_key: PRNG key for randomness\n",
        "\n",
        "    Returns:\n",
        "        next token index\n",
        "    \"\"\"\n",
        "    # Get model predictions\n",
        "    logits = state.apply_fn({'params': state.params}, input_ids, training=False)\n",
        "\n",
        "    # Take logits for the last position\n",
        "    next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
        "\n",
        "    # Apply sampling strategy\n",
        "    if use_nucleus:\n",
        "        next_token = nucleus_sampling(next_token_logits, p=nucleus_p, temperature=temperature, rng_key=rng_key)\n",
        "    else:\n",
        "        next_token = top_k_sampling(next_token_logits, k=top_k, temperature=temperature, rng_key=rng_key)\n",
        "\n",
        "    return next_token\n",
        "\n",
        "def generate_text(state, prompt, tokenizer, max_length=config.max_seq_len, temperature=0.7, top_k=500,\n",
        "                 use_nucleus=True, nucleus_p=0.95, seed=42, stop_at_eos=True, verbose=False):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model with advanced sampling.\n",
        "\n",
        "    Args:\n",
        "        state: training state with model parameters\n",
        "        prompt: input text string to start generation\n",
        "        tokenizer: tokenizer for encoding/decoding\n",
        "        max_length: maximum number of tokens to generate\n",
        "        temperature: sampling temperature (0.1 = conservative, 1.0 = balanced, 2.0 = creative)\n",
        "        top_k: number of top candidates for top-k sampling (lower = more focused)\n",
        "        use_nucleus: whether to use nucleus (top-p) sampling instead of top-k\n",
        "        nucleus_p: probability threshold for nucleus sampling (0.9 = balanced)\n",
        "        seed: random seed for reproducibility\n",
        "        stop_at_eos: whether to stop generation at EOS token\n",
        "        verbose: whether to print generation progress\n",
        "\n",
        "    Returns:\n",
        "        generated text string\n",
        "    \"\"\"\n",
        "    # Initialize random key\n",
        "    rng_key = jax.random.PRNGKey(seed)\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='np')\n",
        "    input_ids = jnp.array(input_ids).reshape(1, -1)  # [1, seq_len]\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   Generating text with:\")\n",
        "        print(f\"   Temperature: {temperature}\")\n",
        "        print(f\"   {'Nucleus (top-p)' if use_nucleus else 'Top-k'}: {nucleus_p if use_nucleus else top_k}\")\n",
        "        print(f\"   Max length: {max_length}\")\n",
        "        print(f\"   Prompt: '{prompt}'\")\n",
        "        print(\"    Generation:\")\n",
        "        print(prompt, end=\"\")\n",
        "\n",
        "    for i in range(max_length):\n",
        "        # Split the random key for this step\n",
        "        rng_key, step_key = jax.random.split(rng_key)\n",
        "\n",
        "        # Generate next token\n",
        "        next_token = generate_next_token(\n",
        "            state, input_ids,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            use_nucleus=use_nucleus,\n",
        "            nucleus_p=nucleus_p,\n",
        "            rng_key=step_key\n",
        "        )\n",
        "\n",
        "        # Convert to Python int\n",
        "        next_token = int(next_token)\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # Check for EOS token\n",
        "        if stop_at_eos and next_token == tokenizer.eos_token_id:\n",
        "            if verbose:\n",
        "                print(\" [EOS]\")\n",
        "            break\n",
        "\n",
        "        # Decode and print the new token if verbose\n",
        "        if verbose:\n",
        "            token_text = tokenizer.decode([next_token])\n",
        "            print(token_text, end=\"\", flush=True)\n",
        "\n",
        "        # Update input_ids for next iteration\n",
        "        next_token_array = jnp.array([[next_token]])\n",
        "        input_ids = jnp.concatenate([input_ids, next_token_array], axis=1)\n",
        "\n",
        "        # Truncate if sequence gets too long (to fit model's max_seq_len)\n",
        "        if input_ids.shape[1] > config.max_seq_len:\n",
        "            input_ids = input_ids[:, -config.max_seq_len:]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n Generation complete!\")\n",
        "\n",
        "    # Decode the full generated text\n",
        "    full_text = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c9c381",
      "metadata": {
        "id": "66c9c381"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing and collate function\n",
        "def collate(batch):\n",
        "    \"\"\"Collate function for DataLoader to handle TinyStories data.\"\"\"\n",
        "    # Extract text from batch\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Tokenize all texts\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        max_length=config.max_seq_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='np'\n",
        "    )\n",
        "\n",
        "    return encoded['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae229e08",
      "metadata": {
        "id": "ae229e08"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate,\n",
        "    # num_workers=int(os.cpu_count() / 2)\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    # num_workers=int(os.cpu_count() / 2)\n",
        ")\n",
        "\n",
        "len(next(iter(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4b1f67",
      "metadata": {
        "id": "1d4b1f67"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_to_file(text, step):\n",
        "\n",
        "    dir = './generated_texts'\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "    with open('generated_texts/{step}.txt', 'w') as f:\n",
        "        f.writelines(text + \"\\n\\n\")\n",
        "\n",
        "def train(resume_from_checkpoint=False, checkpoint_dir=\"./checkpoints\", save_every=1000):\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=\"smoljax-gpt\",\n",
        "        config={\n",
        "            \"vocab_size\": config.vocab_size,\n",
        "            \"max_seq_len\": config.max_seq_len,\n",
        "            \"d_model\": config.d_model,\n",
        "            \"num_layers\": config.num_layers,\n",
        "            \"num_heads\": config.num_heads,\n",
        "            \"d_ff\": config.d_ff,\n",
        "            \"dropout_rate\": config.dropout_rate,\n",
        "            \"learning_rate\": config.lr,\n",
        "            \"warmup_steps\": config.warmup_steps,\n",
        "            \"total_steps\": config.total_steps,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"num_epochs\": config.num_epochs,\n",
        "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
        "            \"save_every\": save_every,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate,\n",
        "        # num_workers=int(os.cpu_count() / 2)\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate,\n",
        "        # num_workers=int(os.cpu_count() / 2)\n",
        "    )\n",
        "\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    train_state = create_train_state(rng, config)\n",
        "\n",
        "    # Initialize variables\n",
        "    start_step = 0\n",
        "    tokens_processed = 0\n",
        "\n",
        "    # Try to resume from checkpoint if requested\n",
        "    if resume_from_checkpoint:\n",
        "        print(\"Checking for existing checkpoints...\")\n",
        "        available_checkpoints = get_checkpoint_info(checkpoint_dir)\n",
        "\n",
        "        if available_checkpoints:\n",
        "            print(f\"Found checkpoints at steps: {available_checkpoints}\")\n",
        "            restored_state, start_step = load_checkpoint(checkpoint_dir, train_state)\n",
        "\n",
        "            if restored_state is not None:\n",
        "                train_state = restored_state\n",
        "                print(f\"Resuming training from step {start_step}\")\n",
        "\n",
        "                # Estimate tokens processed (rough approximation)\n",
        "                tokens_processed = start_step * config.batch_size * config.max_seq_len\n",
        "                print(f\"Estimated tokens processed so far: {tokens_processed:,}\")\n",
        "            else:\n",
        "                print(\"Failed to load checkpoint, starting from scratch\")\n",
        "        else:\n",
        "            print(\"No checkpoints found, starting fresh training\")\n",
        "\n",
        "    # Log model summary to wandb\n",
        "    total_params = sum([param.size for param in jax.tree_util.tree_leaves(train_state.params)])\n",
        "    wandb.log({\n",
        "        \"model/total_parameters\": total_params,\n",
        "        \"model/model_size_mb\": total_params * 2 / (1024**2),  # bfloat16 = 2 bytes\n",
        "        \"checkpoint/resume_from\": start_step\n",
        "    })\n",
        "\n",
        "    # Log detailed training summary\n",
        "    log_training_summary(train_state, config, total_params, tokens_processed)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = config.num_epochs\n",
        "    state = train_state.replace(step=start_step)  # Set correct step for LR schedule\n",
        "    global_step = start_step\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        train_losses = []\n",
        "        train_grad_norms = []\n",
        "\n",
        "        # Create data iterator\n",
        "        # train_data_iterator = prefetch_to_device(iter(train_loader), 2, None)\n",
        "        # val_data_iterator = prefetch_to_device(iter(val_loader), 2, None)\n",
        "        train_data_iterator = iter(train_loader)\n",
        "        val_data_iterator = iter(val_loader)\n",
        "\n",
        "        # Progress bar tracks optimization steps, not data batches\n",
        "        pbar = tqdm(range(config.total_steps), desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for step_idx in pbar:\n",
        "\n",
        "            # Initialize accumulation variables for this step\n",
        "            step_losses = []\n",
        "            step_grad_norms = []\n",
        "            rng, step_key = jax.random.split(rng)\n",
        "\n",
        "            # Gradient accumulation loop\n",
        "            for micro_step in range(config.gradient_accumulation_steps):\n",
        "              try:\n",
        "                  batch = next(train_data_iterator)\n",
        "                  # batch = batch[0]  # Unpack from tuple\n",
        "              except StopIteration:\n",
        "                  # train_data_iterator = prefetch_to_device(iter(train_loader), 2, None)\n",
        "                  train_data_iterator = iter(train_loader)\n",
        "                  batch = next(train_data_iterator)\n",
        "                  # batch = batch[0]  # Unpack from tuple\n",
        "              # print(batch)\n",
        "              # Convert to JAX array\n",
        "              # batch = jnp.array(batch)\n",
        "              batch = jnp.array(batch)\n",
        "              batch = jax.device_put(batch, jax.devices()[0])  # Manual device placement\n",
        "              # Count tokens (excluding padding)\n",
        "              batch_tokens = jnp.sum(batch != tokenizer.pad_token_id)\n",
        "              tokens_processed += int(batch_tokens)\n",
        "\n",
        "              state, loss, grad_norm = train_step_accum(state, batch, step_key)\n",
        "              step_losses.append(float(loss))\n",
        "              # print(grad_norm)\n",
        "              grad_norm = float(grad_norm)\n",
        "              # print(grad_norm)\n",
        "              # if micro_step == config.gradient_accumulation_steps - 1 and grad_norm is not None:\n",
        "              if grad_norm > 0.0:\n",
        "                  # print(\"Final: \", grad_norm)\n",
        "                  step_grad_norms.append(grad_norm)\n",
        "            # print(f\"Gradine Accumulation Running: ( {micro_step} / {config.gradient_accumulation_steps })\")\n",
        "            # Average the losses and grad norms from accumulation steps\n",
        "            avg_step_loss = np.mean(step_losses)\n",
        "            # avg_step_grad_norm = np.mean(step_grad_norms) if step_grad_norms else 0.0\n",
        "\n",
        "            train_losses.append(avg_step_loss)\n",
        "            train_grad_norms.append(step_grad_norms)\n",
        "\n",
        "            # if grad_norm is not None:\n",
        "            #         step_grad_norms.append(float(avg_step_grad_norm))\n",
        "            global_step += 1\n",
        "\n",
        "            # Get current learning rate\n",
        "            # current_lr = float(state.opt_state[1].hyperparams['learning_rate'])\n",
        "            current_lr = create_learning_rate_schedule()(state.step)\n",
        "\n",
        "            # Log training metrics to wandb\n",
        "            wandb.log({\n",
        "                \"train/loss\": avg_step_loss,\n",
        "                \"train/grad_norm\": np.mean(step_grad_norms),\n",
        "                \"train/learning_rate\": current_lr,\n",
        "                \"train/tokens_processed\": tokens_processed,\n",
        "                \"train/epoch\": epoch + 1,\n",
        "                \"train/batch_size\": config.batch_size,\n",
        "                \"train/step\": global_step\n",
        "            }, step=global_step)\n",
        "\n",
        "            # Save checkpoint every save_every steps\n",
        "            if global_step % save_every == 0:\n",
        "                # save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "                # Also save model-only version for inference\n",
        "                save_model_for_inference(\n",
        "                    state,\n",
        "                    model_dir=\"./saved_models\",\n",
        "                    model_name=f\"smoljax_gpt_step_{global_step}\"\n",
        "                )\n",
        "\n",
        "                # # Log checkpoint info to wandb\n",
        "                # wandb.log({\n",
        "                #     \"checkpoint/last_saved_step\": global_step,\n",
        "                #     \"checkpoint/tokens_at_save\": tokens_processed\n",
        "                # }, step=global_step)\n",
        "\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Step {global_step}/{config.total_steps} [Train]\")\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{avg_step_loss:.4f}\",\n",
        "                \"grad_norm\": f\"{np.mean(step_grad_norms):.4f}\",\n",
        "                \"lr\": f\"{current_lr:.6f}\",\n",
        "                \"tokens\": f\"{tokens_processed:,}\",\n",
        "                \"micro_batches\": f\"{config.gradient_accumulation_steps}\"\n",
        "            })\n",
        "\n",
        "            # Break if we've reached the total steps\n",
        "            if global_step >= config.total_steps:\n",
        "                print(f\"\\nReached maximum steps ({config.total_steps}), stopping training...\")\n",
        "                break\n",
        "\n",
        "            # Break out of epoch loop if we've reached max steps\n",
        "            if global_step >= config.total_steps:\n",
        "                break\n",
        "\n",
        "            # Validation\n",
        "            val_losses = []\n",
        "            # val_accs = []\n",
        "\n",
        "            # Simple validation without iterator complexity\n",
        "            # pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "            if(global_step % config.eval_steps == 0):\n",
        "              for batch in tqdm(range(config.eval_steps), desc='Validation running...'):\n",
        "                  try:\n",
        "                    batch = next(val_data_iterator)\n",
        "                    # batch = batch[0]  # Unpack from tuple\n",
        "                  except StopIteration:\n",
        "                    # val_data_iterator = prefetch_to_device(iter(val_loader), 2, None)\n",
        "                    val_data_iterator = iter(val_loader)\n",
        "                    batch = next(val_data_iterator)\n",
        "                    # batch = batch[0]  # Unpack from tuple\n",
        "\n",
        "\n",
        "                  batch = jnp.array(batch)\n",
        "                  batch = jax.device_put(batch, jax.devices()[0])  # Manual device placement\n",
        "                  loss, acc = eval_step(state, batch)\n",
        "                  val_losses.append(float(loss))\n",
        "                  # val_accs.append(float(acc))\n",
        "\n",
        "                  # Update progress bar\n",
        "                  pbar.set_postfix({\n",
        "                      \"loss val\": f\"{loss:.4f}\",\n",
        "                      # \"acc\": f\"{acc:.4f}\"\n",
        "                  })\n",
        "\n",
        "            # Calculate epoch metrics\n",
        "            avg_train_loss = np.mean(train_losses)\n",
        "            avg_train_grad_norm = np.mean(train_grad_norms)\n",
        "            if len(val_losses) > 0:\n",
        "              avg_val_loss = np.mean(val_losses)\n",
        "            else:\n",
        "              avg_val_loss = -1\n",
        "            # avg_val_acc = np.mean(val_accs)\n",
        "\n",
        "            # Log epoch metrics to wandb\n",
        "            wandb.log({\n",
        "                \"epoch/train_loss\": avg_train_loss,\n",
        "                \"epoch/val_loss\": avg_val_loss,\n",
        "                # \"epoch/val_accuracy\": avg_val_acc,\n",
        "                \"epoch/train_grad_norm\": avg_train_grad_norm,\n",
        "                \"epoch/epoch\": epoch + 1\n",
        "            }, step=global_step)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.4f}, Grad Norm: {avg_train_grad_norm:.4f}\")\n",
        "            # print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
        "            print(f\"  Tokens Processed: {tokens_processed:,}\")\n",
        "            print(f\"  Global Step: {global_step}\")\n",
        "\n",
        "            # Save checkpoint at end of each epoch\n",
        "            # save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "    # Save final checkpoint\n",
        "    # print(\"\\n Saving final checkpoint...\")\n",
        "    # save_checkpoint(state, global_step, checkpoint_dir)\n",
        "\n",
        "    # Generate some text\n",
        "    print(\"\\nGenerating text...\")\n",
        "    generated = generate_text(state, \"The future of artificial intelligence\", tokenizer, max_length=50)\n",
        "    print(f\"Generated: {generated}\")\n",
        "    save_to_file(generated, global_step)\n",
        "\n",
        "    # Log final generation to wandb\n",
        "    wandb.log({\n",
        "        \"generation/sample_text\": generated,\n",
        "        \"generation/prompt\": \"The future of artificial intelligence\",\n",
        "        \"training/final_step\": global_step,\n",
        "        \"training/final_tokens\": tokens_processed\n",
        "    })\n",
        "\n",
        "    # Log final training summary\n",
        "    log_training_summary(state, config, total_params, tokens_processed)\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f407e8",
      "metadata": {
        "id": "22f407e8"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tawnWkOgHONx"
      },
      "id": "tawnWkOgHONx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}