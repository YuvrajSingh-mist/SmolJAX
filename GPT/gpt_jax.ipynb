{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ICLM60p0rP9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ICLM60p0rP9",
    "outputId": "e67bff44-635c-45c2-c43c-7b399fd927bb"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install tqdm\n",
    "!pip install jax[cuda12]\n",
    "!pip install optax\n",
    "!pip install flax\n",
    "!pip install datasets\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fd62f",
   "metadata": {
    "id": "151fd62f"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.jax_utils import prefetch_to_device\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.training import checkpoints\n",
    "from flax import struct\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Any\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZdBPFYAKiv0F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdBPFYAKiv0F",
    "outputId": "a7fc2241-6eea-4444-d166-32849fb66699"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# Check TPU availability and setup\n",
    "def check_tpu_setup():\n",
    "    \"\"\"Check if TPU is available and properly configured.\"\"\"\n",
    "    try:\n",
    "        # Check if we're running on TPU\n",
    "        if jax.devices('tpu'):\n",
    "            print(\"✅ TPU detected!\")\n",
    "            devices = jax.devices('tpu')\n",
    "            print(f\"   TPU devices: {len(devices)}\")\n",
    "            for i, device in enumerate(devices):\n",
    "                print(f\"   Device {i}: {device}\")\n",
    "\n",
    "            # Print TPU-specific info\n",
    "            print(f\"   JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "            print(f\"   Total TPU cores: {jax.device_count()}\")\n",
    "            print(f\"   Local devices: {jax.local_device_count()}\")\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ No TPU devices found\")\n",
    "            print(f\"   Available devices: {jax.devices()}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking TPU: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check current setup\n",
    "print(\"Device Information:\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Platform: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "\n",
    "# Check TPU specifically\n",
    "has_tpu = check_tpu_setup()\n",
    "\n",
    "# Test a simple computation\n",
    "test_array = jnp.array([1, 2, 3, 4, 5])\n",
    "result = jnp.sum(test_array ** 2)\n",
    "print(f\"\\nTest computation result: {result}\")\n",
    "print(f\"Computation device: {result.device}\")\n",
    "\n",
    "print(\"jax.devices():\", jax.devices())\n",
    "print(\"jax.device_count():\", jax.device_count())\n",
    "print(\"jax.local_device_count():\", jax.local_device_count())\n",
    "print(\"jax.process_count():\", jax.process_count())\n",
    "print(\"jax.process_index():\", jax.process_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PZzi9sSvC5C3",
   "metadata": {
    "id": "PZzi9sSvC5C3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['FLAX_USE_LEGACY_CHECKPOINTS'] = '1'  # Force legacy checkpoints to avoid Orbax issues\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80NfOZiEDOyb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80NfOZiEDOyb",
    "outputId": "0f29afbb-4cf9-4784-b75b-a19b537af898"
   },
   "outputs": [],
   "source": [
    "!hf auth login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc054c",
   "metadata": {
    "id": "cafc054c"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=\"\")\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012c592",
   "metadata": {
    "id": "d012c592"
   },
   "outputs": [],
   "source": [
    "# Configuration class for model parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = tokenizer.vocab_size + 47\n",
    "    max_seq_len: int = 256\n",
    "    d_model: int = 768\n",
    "    num_layers: int = 8\n",
    "    num_heads: int = 8\n",
    "    d_ff: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    lr: float = 6e-4\n",
    "    min_lr: float = 0.1 * lr\n",
    "    warmup_steps: int = 700\n",
    "    total_steps: int = 20000\n",
    "    batch_size: int = 256\n",
    "    required_bsz_tokens: int = 524288\n",
    "    gradient_accumulation_steps: int = int(required_bsz_tokens // (batch_size * max_seq_len))\n",
    "    mixed_precision: bool = True\n",
    "    num_epochs: int = 1\n",
    "    eval_steps: int = 200\n",
    "\n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geW4X-AIp68l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geW4X-AIp68l",
    "outputId": "eb7b5930-3f1d-44a8-d3c4-28e228bbf845"
   },
   "outputs": [],
   "source": [
    "from jax import config as jax_config\n",
    "\n",
    "# Set matmul precision based on mixed_precision setting\n",
    "if config.mixed_precision:\n",
    "    jax_config.update(\"jax_default_matmul_precision\", \"bfloat16\")\n",
    "    print(\"Set JAX matmul precision to bfloat16 (mixed precision enabled)\")\n",
    "else:\n",
    "    jax_config.update(\"jax_default_matmul_precision\", \"float32\")\n",
    "    print(\"Set JAX matmul precision to float32 (mixed precision disabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a08a28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30a08a28",
    "outputId": "4f1a0332-7f59-4893-a559-4fe79be7fadd"
   },
   "outputs": [],
   "source": [
    "# Helper function to get the appropriate dtype based on mixed_precision setting\n",
    "def get_dtype():\n",
    "    \"\"\"Return bfloat16 if mixed_precision is True, else float32.\"\"\"\n",
    "    return jnp.bfloat16 if config.mixed_precision else jnp.float32\n",
    "\n",
    "print(f\"Using dtype: {get_dtype()} (mixed_precision={config.mixed_precision})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N3D8_qec1GTW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3D8_qec1GTW",
    "outputId": "dbb75ab8-6b3d-4b46-d8fa-c2b9ffe8d611"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502276f0",
   "metadata": {
    "id": "502276f0"
   },
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    grad_accum: Any = None\n",
    "    accum_step: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa95aed",
   "metadata": {
    "id": "2fa95aed"
   },
   "outputs": [],
   "source": [
    "#Loading TinyStories dataset from Huggingface\n",
    "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", token='')\n",
    "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16494b05",
   "metadata": {
    "id": "16494b05"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "\n",
    "        # Proper initialization for attention layers\n",
    "        self.d_Q = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_K = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_V = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_O = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        B, T, C = x.shape\n",
    "        query = self.d_Q(x)\n",
    "        key = self.d_K(x)\n",
    "        value = self.d_V(x)\n",
    "\n",
    "        # Proper attention scaling using head_size\n",
    "        weights = jnp.matmul(query, key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
    "\n",
    "        # Better causal mask using -inf\n",
    "        mask = jnp.tril(jnp.ones((T, T)))\n",
    "        weights = jnp.where(mask == 0, -jnp.inf, weights)\n",
    "\n",
    "        weights = nn.softmax(weights, axis=-1)\n",
    "        # weights = self.dropout(weights, deterministic=not training)  # Apply dropout to attention weights\n",
    "\n",
    "        out = jnp.matmul(weights, value)\n",
    "        out = self.d_O(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb96ba1",
   "metadata": {
    "id": "6eb96ba1"
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [Attention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "\n",
    "        # Proper initialization for output projection\n",
    "        self.linear = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd804b6",
   "metadata": {
    "id": "1fd804b6"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        # Proper initialization for MLP layers\n",
    "        self.fc1 = nn.Dense(\n",
    "            features=self.d_ff,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.fc2 = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x, deterministic=not training)  # Remove duplicate GELU\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2a846",
   "metadata": {
    "id": "c1b2a846"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm(dtype=get_dtype())\n",
    "        self.ln2 = nn.LayerNorm(dtype=get_dtype())\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        attn =  self.attention(self.ln1(x), training)\n",
    "        # x += attn\n",
    "        x = x + attn * ((2 * config.num_layers ** -0.5))\n",
    "        mlp_out = self.mlp(self.ln2(x), training)\n",
    "        # x += mlp_out\n",
    "        x = x + mlp_out * (2 * (config.num_layers ** -0.5))\n",
    "        # x = x * (config.num_layers ** -0.5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83412e4",
   "metadata": {
    "id": "c83412e4"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "    vocab_size: int = config.vocab_size\n",
    "    seq_len: int = config.max_seq_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model, dtype=get_dtype(), embedding_init=nn.initializers.normal(stddev=0.02))\n",
    "        self.positional_embedding = self.param(\n",
    "            \"positional_embeddings\",  # name\n",
    "            lambda key: jax.random.normal(key, (1, self.seq_len, self.d_model), dtype=get_dtype()) * 0.01\n",
    "        )\n",
    "        self.decoder = [TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(config.num_layers)]\n",
    "        self.linear_out = nn.Dense(features=self.vocab_size, dtype=get_dtype(), kernel_init=nn.initializers.normal(stddev=0.02), bias_init=nn.initializers.zeros)  # Zero bias initialization\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        B,T = x.shape\n",
    "        embeds = self.embedding_table(x)  # (B,T,d_model)\n",
    "        C = embeds.shape[-1]\n",
    "        pos_embeds = self.positional_embedding[:, :T, :]  # (1,T,d_model)\n",
    "        x = embeds + pos_embeds  # (B,T,d_model)\n",
    "        # pad_mask = (x != tokenizer.pad_token_id).astype(get_dtype())\n",
    "        # x = x * pad_mask\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, training=training)\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb78ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6eb78ff",
    "outputId": "7bb79319-6e2c-4695-f299-b4b60d613cc5"
   },
   "outputs": [],
   "source": [
    "# Add this cell to inspect the model summary like torchsummary\n",
    "\n",
    "from flax.linen import tabulate\n",
    "import jax\n",
    "\n",
    "# Initialize model\n",
    "model = GPT()\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # Dummy input for tabulation\n",
    "\n",
    "# Tabulate the model structure\n",
    "tabulate_fn = tabulate(model, key, console_kwargs={'width': 120})\n",
    "\n",
    "# Count total parameters\n",
    "params = model.init(key, x)['params']\n",
    "total_params = sum(jax.tree_util.tree_leaves(jax.tree.map(lambda arr: arr.size, params)))\n",
    "\n",
    "# Get raw summary and clean ANSI codes\n",
    "raw_summary = tabulate_fn(x, training=True)\n",
    "# Remove ANSI color codes for clean logging\n",
    "clean_summary = re.sub(r'\\x1b\\[[0-9;]*m', '', raw_summary)\n",
    "\n",
    "# Save to log file with clean formatting\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"GPT MODEL ARCHITECTURE SUMMARY\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"Model Configuration:\\n\")\n",
    "    f.write(f\"  - Vocabulary Size: {config.vocab_size:,}\\n\")\n",
    "    f.write(f\"  - Max Sequence Length: {config.max_seq_len}\\n\")\n",
    "    f.write(f\"  - Model Dimension: {config.d_model}\\n\")\n",
    "    f.write(f\"  - Number of Layers: {config.num_layers}\\n\")\n",
    "    f.write(f\"  - Number of Heads: {config.num_heads}\\n\")\n",
    "    f.write(f\"  - Feed Forward Dimension: {config.d_ff}\\n\")\n",
    "    f.write(f\"  - Dropout Rate: {config.dropout_rate}\\n\\n\")\n",
    "    f.write(\"Detailed Layer Information:\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(clean_summary)\n",
    "\n",
    "print(f\"Model summary saved to model_summary.txt\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 2 / (1024**2):.1f} MB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b39759",
   "metadata": {
    "id": "57b39759"
   },
   "outputs": [],
   "source": [
    "def create_learning_rate_schedule():\n",
    "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
    "    # Use values from config\n",
    "    max_lr = config.lr  # 6e-4\n",
    "    min_lr = config.min_lr\n",
    "    warmup_steps = config.warmup_steps  # 700 (or can override to 715)\n",
    "    max_steps = config.total_steps  # 20000 (or can override to 19073)\n",
    "    \n",
    "    def get_lr(it):\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < warmup_steps:\n",
    "            return max_lr * (it + 1) / warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > max_steps:\n",
    "            return min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + jnp.cos(jnp.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n",
    "        return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return get_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a1842",
   "metadata": {
    "id": "687a1842"
   },
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    labels = labels[:, 1:]\n",
    "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    pad_mask = (labels != tokenizer.pad_token_id)\n",
    "    loss = jnp.where(pad_mask, loss, 0.0)\n",
    "    return loss.sum () / pad_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd1398",
   "metadata": {
    "id": "23fd1398"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "    model = GPT()\n",
    "\n",
    "    # Initialize parameters\n",
    "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
    "    params = model.init(rng, dummy_input)['params']\n",
    "\n",
    "    # Create learning rate schedule\n",
    "    lr_schedule = create_learning_rate_schedule()\n",
    "\n",
    "    # Create optimizer with stronger gradient clipping and better settings\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),  # Much stronger clipping (was 1.0)\n",
    "        optax.adamw(\n",
    "            learning_rate=lr_schedule,\n",
    "            b1=0.9,\n",
    "            b2=0.95,\n",
    "            weight_decay=0.01,  # Reduced weight decay (was 0.1)\n",
    "            eps=1e-9  # Added epsilon for numerical stability\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af46bc",
   "metadata": {
    "id": "79af46bc"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, step):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "\n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "\n",
    "    # Compute gradient norm for logging\n",
    "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
    "\n",
    "    # Update the parameters\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7ffe7",
   "metadata": {
    "id": "35c7ffe7"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_accum(state, batch, step):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "\n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    # Compute gradient norm for logging\n",
    "\n",
    "    if state.grad_accum is None:\n",
    "        \n",
    "         \n",
    "\n",
    "        state = state.replace(\n",
    "            grad_accum=jax.tree_util.tree_map(jnp.zeros_like, grads),\n",
    "            accum_step=0,\n",
    "        )\n",
    "    new_accum = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, state.grad_accum, grads)\n",
    "    new_step = state.accum_step + 1\n",
    "\n",
    "    # print(new_accum)\n",
    "    # print(new_step)\n",
    "\n",
    "    def apply_update(_):\n",
    "        mean_grads = jax.tree_util.tree_map(lambda g: g / config.gradient_accumulation_steps, new_accum)\n",
    "        # print(\"Grads: \", mean_grads)\n",
    "        grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(mean_grads)]))\n",
    "        # print(\"Here: \", jnp.array(grad_norm, float))\n",
    "        new_state = state.apply_gradients(grads = mean_grads)\n",
    "        new_grad_accum = jax.tree_util.tree_map(jnp.zeros_like, new_accum)\n",
    "\n",
    "        return new_state.replace(grad_accum=new_grad_accum, accum_step=0), grad_norm\n",
    "\n",
    "    def carry_forward(_):\n",
    "\n",
    "        # dummy_grads = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
    "\n",
    "        return state.replace(grad_accum=new_accum, accum_step=new_step), -1.0\n",
    "\n",
    "\n",
    "    state, grad_norm = jax.lax.cond(new_step == config.gradient_accumulation_steps, apply_update, carry_forward, operand=None)\n",
    "\n",
    "    return state, loss, grad_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fc0cc",
   "metadata": {
    "id": "f95fc0cc"
   },
   "outputs": [],
   "source": [
    "# JIT-compiled evaluation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch, step):\n",
    "    \"\"\"Single evaluation step.\"\"\"\n",
    "    logits = state.apply_fn({'params': state.params}, batch, training=False, rngs={'dropout': step})\n",
    "    loss = compute_ce_loss(logits, batch)\n",
    "\n",
    "    return loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c34940",
   "metadata": {
    "id": "63c34940"
   },
   "outputs": [],
   "source": [
    "# Helper function to create wandb summary table\n",
    "def log_training_summary(state, config, total_params, tokens_processed):\n",
    "    \"\"\"Log a comprehensive training summary to wandb.\"\"\"\n",
    "\n",
    "    # Create a summary table (ensure all values are strings for wandb compatibility)\n",
    "    summary_data = [\n",
    "        [\"Model\", \"SmolJAX GPT\"],\n",
    "        [\"Total Parameters\", f\"{total_params:,}\"],\n",
    "        [\"Model Size (MB)\", f\"{total_params * 2 / (1024**2):.1f}\"],\n",
    "        [\"Vocabulary Size\", f\"{config.vocab_size:,}\"],\n",
    "        [\"Max Sequence Length\", f\"{config.max_seq_len}\"],\n",
    "        [\"Model Dimension\", f\"{config.d_model}\"],\n",
    "        [\"Number of Layers\", f\"{config.num_layers}\"],\n",
    "        [\"Number of Heads\", f\"{config.num_heads}\"],\n",
    "        [\"Feed Forward Dimension\", f\"{config.d_ff}\"],\n",
    "        [\"Dropout Rate\", f\"{config.dropout_rate}\"],\n",
    "        [\"Learning Rate\", f\"{config.lr}\"],\n",
    "        [\"Batch Size\", f\"{config.batch_size}\"],\n",
    "        [\"Total Epochs\", f\"{config.num_epochs}\"],\n",
    "        [\"Tokens Processed\", f\"{tokens_processed:,}\"],\n",
    "        [\"Training Step\", f\"{int(state.step)}\"]\n",
    "    ]\n",
    "\n",
    "    # Create wandb table\n",
    "    table = wandb.Table(\n",
    "        columns=[\"Metric\", \"Value\"],\n",
    "        data=summary_data\n",
    "    )\n",
    "\n",
    "    wandb.log({\"training_summary\": table})\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a98639",
   "metadata": {
    "id": "e4a98639"
   },
   "outputs": [],
   "source": [
    "# Checkpoint management functions\n",
    "def save_checkpoint(state, step, checkpoint_dir=\"./checkpoints\", keep=5):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Save the checkpoint\n",
    "    checkpoints.save_checkpoint(\n",
    "        ckpt_dir=checkpoint_dir,\n",
    "        target=state,\n",
    "        step=step,\n",
    "        keep=keep,  # Keep only the last 5 checkpoints\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    print(f\"Checkpoint saved at step {step} in {checkpoint_dir}\")\n",
    "\n",
    "    # Log to wandb if available\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"checkpoint/step\": step,\n",
    "            \"checkpoint/saved\": 1\n",
    "        }, step=step)\n",
    "\n",
    "def load_checkpoint(checkpoint_dir=\"./checkpoints\", state=None):\n",
    "    \"\"\"Load the latest checkpoint.\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        print(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        return None, 0\n",
    "\n",
    "    # Check if there are any checkpoints\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return None, 0\n",
    "\n",
    "    try:\n",
    "        # Load the latest checkpoint\n",
    "        restored_state = checkpoints.restore_checkpoint(\n",
    "            ckpt_dir=checkpoint_dir,\n",
    "            target=state\n",
    "        )\n",
    "\n",
    "        # Get the step number from the checkpoint\n",
    "        latest_step = checkpoints.latest_checkpoint(checkpoint_dir)\n",
    "        if latest_step:\n",
    "            step = int(latest_step.split('_')[-1])\n",
    "            print(f\"Checkpoint loaded from step {step}\")\n",
    "            return restored_state, step\n",
    "        else:\n",
    "            print(f\"Could not determine step from checkpoint\")\n",
    "            return None, 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def get_checkpoint_info(checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Get information about available checkpoints.\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return []\n",
    "\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')]\n",
    "    checkpoint_steps = []\n",
    "\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            step = int(file.split('_')[-1])\n",
    "            checkpoint_steps.append(step)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return sorted(checkpoint_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e41b1",
   "metadata": {
    "id": "522e41b1"
   },
   "outputs": [],
   "source": [
    "# Model-only saving/loading functions for inference\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def save_model_for_inference(state, model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Save only model parameters and config for inference (much smaller files).\"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save just the parameters (no optimizer state)\n",
    "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
    "    with open(params_path, 'wb') as f:\n",
    "        pickle.dump(state.params, f)\n",
    "\n",
    "    # Save model configuration\n",
    "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
    "    config_dict = {\n",
    "        \"vocab_size\": config.vocab_size,\n",
    "        \"max_seq_len\": config.max_seq_len,\n",
    "        \"d_model\": config.d_model,\n",
    "        \"num_layers\": config.num_layers,\n",
    "        \"num_heads\": config.num_heads,\n",
    "        \"d_ff\": config.d_ff,\n",
    "        \"dropout_rate\": config.dropout_rate\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    # Save tokenizer info\n",
    "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
    "    tokenizer_info = {\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"pad_token\": tokenizer.pad_token,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token\": tokenizer.eos_token,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    with open(tokenizer_path, 'w') as f:\n",
    "        json.dump(tokenizer_info, f, indent=2)\n",
    "\n",
    "    print(f\"   Model saved for inference:\")\n",
    "    print(f\"   Parameters: {params_path}\")\n",
    "    print(f\"   Config: {config_path}\")\n",
    "    print(f\"   Tokenizer info: {tokenizer_path}\")\n",
    "\n",
    "    # Calculate file sizes\n",
    "    params_size = os.path.getsize(params_path) / (1024**2)  # MB\n",
    "    print(f\"   Model size: {params_size:.1f} MB\")\n",
    "\n",
    "    return params_path, config_path, tokenizer_path\n",
    "\n",
    "def load_model_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Load model parameters and config for inference.\"\"\"\n",
    "\n",
    "    # Load configuration\n",
    "    config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "\n",
    "    # Recreate config object\n",
    "    inference_config = GPTConfig(**config_dict)\n",
    "\n",
    "    # Load parameters\n",
    "    params_path = os.path.join(model_dir, f\"{model_name}_params.pkl\")\n",
    "    if not os.path.exists(params_path):\n",
    "        raise FileNotFoundError(f\"Parameters file not found: {params_path}\")\n",
    "\n",
    "    with open(params_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "    # Load tokenizer info\n",
    "    tokenizer_path = os.path.join(model_dir, f\"{model_name}_tokenizer_info.json\")\n",
    "    tokenizer_info = None\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        with open(tokenizer_path, 'r') as f:\n",
    "            tokenizer_info = json.load(f)\n",
    "\n",
    "    print(f\" Model loaded for inference:\")\n",
    "    print(f\"   Config: {config_dict}\")\n",
    "    if tokenizer_info:\n",
    "        print(f\"   Tokenizer: {tokenizer_info['tokenizer_name']}\")\n",
    "\n",
    "    return params, inference_config, tokenizer_info\n",
    "\n",
    "def create_inference_model(params, inference_config):\n",
    "    \"\"\"Create a model instance for inference (no training state).\"\"\"\n",
    "\n",
    "    # Create model with loaded config\n",
    "    model = GPT(\n",
    "        d_model=inference_config.d_model,\n",
    "        num_heads=inference_config.num_heads,\n",
    "        d_ff=inference_config.d_ff,\n",
    "        dropout_rate=inference_config.dropout_rate,\n",
    "        vocab_size=inference_config.vocab_size,\n",
    "        seq_len=inference_config.max_seq_len\n",
    "    )\n",
    "\n",
    "    # Create apply function with loaded parameters\n",
    "    def inference_apply(inputs, training=False):\n",
    "        return model.apply({'params': params}, inputs, training=training)\n",
    "\n",
    "    return model, inference_apply\n",
    "\n",
    "def load_and_setup_for_inference(model_dir=\"./saved_models\", model_name=\"smoljax_gpt\"):\n",
    "    \"\"\"Complete setup for inference - load everything and return ready-to-use functions.\"\"\"\n",
    "\n",
    "    # Load model components\n",
    "    params, inference_config, tokenizer_info = load_model_for_inference(model_dir, model_name)\n",
    "\n",
    "    # Create model and inference function\n",
    "    model, inference_apply = create_inference_model(params, inference_config)\n",
    "\n",
    "    # Setup tokenizer (you might want to load this separately)\n",
    "    if tokenizer_info and tokenizer_info.get('tokenizer_name') == 'gpt2':\n",
    "        from transformers import AutoTokenizer\n",
    "        inference_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        inference_tokenizer.pad_token = tokenizer_info.get('pad_token', '[PAD]')\n",
    "        print(f\" Tokenizer setup complete\")\n",
    "    else:\n",
    "        inference_tokenizer = None\n",
    "        print(\" No tokenizer info found, you'll need to setup tokenizer manually\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'apply_fn': inference_apply,\n",
    "        'params': params,\n",
    "        'config': inference_config,\n",
    "        'tokenizer': inference_tokenizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6200ac8",
   "metadata": {
    "id": "e6200ac8"
   },
   "outputs": [],
   "source": [
    "# Advanced Text Generation with Top-K and Temperature Sampling\n",
    "import jax.random as random\n",
    "\n",
    "def top_k_sampling(logits, k=550, temperature=0.7, rng_key=None):\n",
    "    \"\"\"\n",
    "    Apply top-k sampling with temperature to logits.\n",
    "\n",
    "    Args:\n",
    "        logits: [vocab_size] array of logits\n",
    "        k: number of top candidates to keep\n",
    "        temperature: sampling temperature (lower = more deterministic)\n",
    "        rng_key: PRNG key for randomness\n",
    "\n",
    "    Returns:\n",
    "        sampled token index\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Get top-k indices and values\n",
    "    top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n",
    "\n",
    "    # Convert to probabilities\n",
    "    top_k_probs = jax.nn.softmax(top_k_logits)\n",
    "\n",
    "    # Sample from the top-k distribution\n",
    "    sampled_idx = jax.random.categorical(rng_key, jnp.log(top_k_probs))\n",
    "\n",
    "    # Return the actual token index\n",
    "    return top_k_indices[sampled_idx]\n",
    "\n",
    "def nucleus_sampling(logits, p=0.95, temperature=0.7, rng_key=None):\n",
    "    \"\"\"\n",
    "    Apply nucleus (top-p) sampling with temperature to logits.\n",
    "\n",
    "    Args:\n",
    "        logits: [vocab_size] array of logits\n",
    "        p: cumulative probability threshold\n",
    "        temperature: sampling temperature\n",
    "        rng_key: PRNG key for randomness\n",
    "\n",
    "    Returns:\n",
    "        sampled token index\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    # logits = logits / temperature\n",
    "\n",
    "    # Convert to probabilities and sort\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    sorted_indices = jnp.argsort(probs)[::-1]  # Sort in descending order\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "\n",
    "    # Find cumulative probabilities\n",
    "    cumsum_probs = jnp.cumsum(sorted_probs)\n",
    "\n",
    "    # Find the cutoff index where cumsum exceeds p\n",
    "    cutoff = jnp.searchsorted(cumsum_probs, p)\n",
    "    cutoff = jnp.maximum(cutoff, 1)  # Keep at least one token\n",
    "\n",
    "    # Keep only tokens within the nucleus\n",
    "    nucleus_indices = sorted_indices[:cutoff]\n",
    "    nucleus_probs = sorted_probs[:cutoff]\n",
    "    nucleus_probs = nucleus_probs / jnp.sum(nucleus_probs)  # Renormalize\n",
    "\n",
    "    # Sample from the nucleus\n",
    "    sampled_idx = jax.random.categorical(rng_key, jnp.log(nucleus_probs))\n",
    "\n",
    "    return nucleus_indices[sampled_idx]\n",
    "\n",
    "@jax.jit\n",
    "def generate_next_token(state, input_ids, temperature=1.0, top_k=50, use_nucleus=False, nucleus_p=0.9, rng_key=None):\n",
    "    \"\"\"\n",
    "    Generate the next token using the model.\n",
    "\n",
    "    Args:\n",
    "        state: training state with model parameters\n",
    "        input_ids: current sequence [batch_size, seq_len]\n",
    "        temperature: sampling temperature\n",
    "        top_k: number of top candidates for top-k sampling\n",
    "        use_nucleus: whether to use nucleus sampling instead of top-k\n",
    "        nucleus_p: probability threshold for nucleus sampling\n",
    "        rng_key: PRNG key for randomness\n",
    "\n",
    "    Returns:\n",
    "        next token index\n",
    "    \"\"\"\n",
    "    # Get model predictions\n",
    "    logits = state.apply_fn({'params': state.params}, input_ids, training=False,  rngs={'dropout': rng_key})\n",
    "\n",
    "    # Take logits for the last position\n",
    "    next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
    "\n",
    "    # Apply sampling strategy\n",
    "    if use_nucleus:\n",
    "        next_token = nucleus_sampling(next_token_logits, p=nucleus_p, temperature=temperature, rng_key=rng_key)\n",
    "    else:\n",
    "        next_token = top_k_sampling(next_token_logits, k=top_k, temperature=temperature, rng_key=rng_key)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "def generate_text(state, prompt, tokenizer, max_length=config.max_seq_len, temperature=0.7, top_k=500,\n",
    "                 use_nucleus=True, nucleus_p=0.95, seed=42, stop_at_eos=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model with advanced sampling.\n",
    "\n",
    "    Args:\n",
    "        state: training state with model parameters\n",
    "        prompt: input text string to start generation\n",
    "        tokenizer: tokenizer for encoding/decoding\n",
    "        max_length: maximum number of tokens to generate\n",
    "        temperature: sampling temperature (0.1 = conservative, 1.0 = balanced, 2.0 = creative)\n",
    "        top_k: number of top candidates for top-k sampling (lower = more focused)\n",
    "        use_nucleus: whether to use nucleus (top-p) sampling instead of top-k\n",
    "        nucleus_p: probability threshold for nucleus sampling (0.9 = balanced)\n",
    "        seed: random seed for reproducibility\n",
    "        stop_at_eos: whether to stop generation at EOS token\n",
    "        verbose: whether to print generation progress\n",
    "\n",
    "    Returns:\n",
    "        generated text string\n",
    "    \"\"\"\n",
    "    # Initialize random key\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='np')\n",
    "    input_ids = jnp.array(input_ids).reshape(1, -1)  # [1, seq_len]\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   Generating text with:\")\n",
    "        print(f\"   Temperature: {temperature}\")\n",
    "        print(f\"   {'Nucleus (top-p)' if use_nucleus else 'Top-k'}: {nucleus_p if use_nucleus else top_k}\")\n",
    "        print(f\"   Max length: {max_length}\")\n",
    "        print(f\"   Prompt: '{prompt}'\")\n",
    "        print(\"    Generation:\")\n",
    "        print(prompt, end=\"\")\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # Split the random key for this step\n",
    "        rng_key, step_key = jax.random.split(rng_key)\n",
    "\n",
    "        # Generate next token\n",
    "        next_token = generate_next_token(\n",
    "            state, input_ids,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            use_nucleus=use_nucleus,\n",
    "            nucleus_p=nucleus_p,\n",
    "            rng_key=step_key\n",
    "        )\n",
    "\n",
    "        # Convert to Python int\n",
    "        next_token = int(next_token)\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        # Check for EOS token\n",
    "        if stop_at_eos and next_token == tokenizer.eos_token_id:\n",
    "            if verbose:\n",
    "                print(\" [EOS]\")\n",
    "            break\n",
    "\n",
    "        # Decode and print the new token if verbose\n",
    "        if verbose:\n",
    "            token_text = tokenizer.decode([next_token])\n",
    "            print(token_text, end=\"\", flush=True)\n",
    "\n",
    "        # Update input_ids for next iteration\n",
    "        next_token_array = jnp.array([[next_token]])\n",
    "        input_ids = jnp.concatenate([input_ids, next_token_array], axis=1)\n",
    "\n",
    "        # Truncate if sequence gets too long (to fit model's max_seq_len)\n",
    "        if input_ids.shape[1] > config.max_seq_len:\n",
    "            input_ids = input_ids[:, -config.max_seq_len:]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n Generation complete!\")\n",
    "\n",
    "    # Decode the full generated text\n",
    "    full_text = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9c381",
   "metadata": {
    "id": "66c9c381"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and collate function\n",
    "def collate(batch):\n",
    "    \"\"\"Collate function for DataLoader to handle TinyStories data.\"\"\"\n",
    "    # Extract text from batch\n",
    "    texts = [item['text'] for item in batch]\n",
    "\n",
    "    # Tokenize all texts\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        max_length=config.max_seq_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    return encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae229e08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae229e08",
    "outputId": "ae251c8c-ce88-4824-efed-801e5d492d40"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    # num_workers=int(os.cpu_count() / 2)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate,\n",
    "    # num_workers=int(os.cpu_count() / 2)\n",
    ")\n",
    "\n",
    "len(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b1f67",
   "metadata": {
    "id": "1d4b1f67"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_to_file(text, step):\n",
    "\n",
    "    dir = './generated_texts'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "    with open('generated_texts/{step}.txt', 'w') as f:\n",
    "        f.writelines(text + \"\\n\\n\")\n",
    "\n",
    "def train(resume_from_checkpoint=False, checkpoint_dir=\"./checkpoints\", save_every=1000):\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"smoljax-gpt\",\n",
    "        config={\n",
    "            \"vocab_size\": config.vocab_size,\n",
    "            \"max_seq_len\": config.max_seq_len,\n",
    "            \"d_model\": config.d_model,\n",
    "            \"num_layers\": config.num_layers,\n",
    "            \"num_heads\": config.num_heads,\n",
    "            \"d_ff\": config.d_ff,\n",
    "            \"dropout_rate\": config.dropout_rate,\n",
    "            \"learning_rate\": config.lr,\n",
    "            \"warmup_steps\": config.warmup_steps,\n",
    "            \"total_steps\": config.total_steps,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"num_epochs\": config.num_epochs,\n",
    "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "            \"save_every\": save_every,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        # num_workers=int(os.cpu_count() / 2)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate,\n",
    "        # num_workers=int(os.cpu_count() / 2)\n",
    "    )\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    train_state = create_train_state(rng, config)\n",
    "\n",
    "    # Initialize variables\n",
    "    start_step = 0\n",
    "    tokens_processed = 0\n",
    "\n",
    "    # Try to resume from checkpoint if requested\n",
    "    if resume_from_checkpoint:\n",
    "        print(\"Checking for existing checkpoints...\")\n",
    "        available_checkpoints = get_checkpoint_info(checkpoint_dir)\n",
    "\n",
    "        if available_checkpoints:\n",
    "            print(f\"Found checkpoints at steps: {available_checkpoints}\")\n",
    "            restored_state, start_step = load_checkpoint(checkpoint_dir, train_state)\n",
    "\n",
    "            if restored_state is not None:\n",
    "                train_state = restored_state\n",
    "                print(f\"Resuming training from step {start_step}\")\n",
    "\n",
    "                # Estimate tokens processed (rough approximation)\n",
    "                tokens_processed = start_step * config.batch_size * config.max_seq_len\n",
    "                print(f\"Estimated tokens processed so far: {tokens_processed:,}\")\n",
    "            else:\n",
    "                print(\"Failed to load checkpoint, starting from scratch\")\n",
    "        else:\n",
    "            print(\"No checkpoints found, starting fresh training\")\n",
    "\n",
    "    # Log model summary to wandb\n",
    "    total_params = sum([param.size for param in jax.tree_util.tree_leaves(train_state.params)])\n",
    "    wandb.log({\n",
    "        \"model/total_parameters\": total_params,\n",
    "        \"model/model_size_mb\": total_params * 2 / (1024**2),  # bfloat16 = 2 bytes\n",
    "        \"checkpoint/resume_from\": start_step\n",
    "    })\n",
    "\n",
    "    # Log detailed training summary\n",
    "    log_training_summary(train_state, config, total_params, tokens_processed)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = config.num_epochs\n",
    "    state = train_state.replace(step=start_step)  # Set correct step for LR schedule\n",
    "    global_step = start_step\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_losses = []\n",
    "        train_grad_norms = []\n",
    "\n",
    "        # Create data iterator\n",
    "        # train_data_iterator = prefetch_to_device(iter(train_loader), 2, None)\n",
    "        # val_data_iterator = prefetch_to_device(iter(val_loader), 2, None)\n",
    "        train_data_iterator = iter(train_loader)\n",
    "        val_data_iterator = iter(val_loader)\n",
    "\n",
    "        # Progress bar tracks optimization steps, not data batches\n",
    "        pbar = tqdm(range(config.total_steps), desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for step_idx in pbar:\n",
    "\n",
    "            # Initialize accumulation variables for this step\n",
    "            step_losses = []\n",
    "            step_grad_norms = []\n",
    "            rng, step_key = jax.random.split(rng)\n",
    "\n",
    "            # Gradient accumulation loop\n",
    "            for micro_step in range(config.gradient_accumulation_steps):\n",
    "              \n",
    "              rng, step_key = jax.random.split(rng)\n",
    "              try:\n",
    "                  batch = next(train_data_iterator)\n",
    "                  # batch = batch[0]  # Unpack from tuple\n",
    "              except StopIteration:\n",
    "                  # train_data_iterator = prefetch_to_device(iter(train_loader), 2, None)\n",
    "                  train_data_iterator = iter(train_loader)\n",
    "                  batch = next(train_data_iterator)\n",
    "                  # batch = batch[0]  # Unpack from tuple\n",
    "              # print(batch)\n",
    "              # Convert to JAX array\n",
    "              # batch = jnp.array(batch)\n",
    "              batch = jnp.array(batch)\n",
    "              batch = jax.device_put(batch, jax.devices()[0])  # Manual device placement\n",
    "              # Count tokens (excluding padding)\n",
    "              batch_tokens = jnp.sum(batch != tokenizer.pad_token_id)\n",
    "              tokens_processed += int(batch_tokens)\n",
    "\n",
    "              state, loss, grad_norm = train_step_accum(state, batch, step_key)\n",
    "              step_losses.append(float(loss))\n",
    "              # print(grad_norm)\n",
    "              grad_norm = float(grad_norm)\n",
    "              # print(grad_norm)\n",
    "              # if micro_step == config.gradient_accumulation_steps - 1 and grad_norm is not None:\n",
    "              if grad_norm > 0.0:\n",
    "                  # print(\"Final: \", grad_norm)\n",
    "                  step_grad_norms.append(grad_norm)\n",
    "            # print(f\"Gradine Accumulation Running: ( {micro_step} / {config.gradient_accumulation_steps })\")\n",
    "            # Average the losses and grad norms from accumulation steps\n",
    "            avg_step_loss = np.mean(step_losses)\n",
    "            # avg_step_grad_norm = np.mean(step_grad_norms) if step_grad_norms else 0.0\n",
    "\n",
    "            train_losses.append(avg_step_loss)\n",
    "            train_grad_norms.append(step_grad_norms)\n",
    "\n",
    "            # if grad_norm is not None:\n",
    "            #         step_grad_norms.append(float(avg_step_grad_norm))\n",
    "            global_step += 1\n",
    "\n",
    "            # Get current learning rate\n",
    "            # current_lr = float(state.opt_state[1].hyperparams['learning_rate'])\n",
    "            current_lr = create_learning_rate_schedule()(state.step)\n",
    "\n",
    "            # Log training metrics to wandb\n",
    "            wandb.log({\n",
    "                \"train/loss\": avg_step_loss,\n",
    "                \"train/grad_norm\": np.mean(step_grad_norms),\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                \"train/tokens_processed\": tokens_processed,\n",
    "                \"train/epoch\": epoch + 1,\n",
    "                \"train/batch_size\": config.batch_size,\n",
    "                \"train/step\": global_step\n",
    "            }, step=global_step)\n",
    "\n",
    "            # Save checkpoint every save_every steps\n",
    "            if global_step % save_every == 0:\n",
    "                # save_checkpoint(state, global_step, checkpoint_dir)\n",
    "\n",
    "                # Also save model-only version for inference\n",
    "                save_model_for_inference(\n",
    "                    state,\n",
    "                    model_dir=\"./saved_models\",\n",
    "                    model_name=f\"smoljax_gpt_step_{global_step}\"\n",
    "                )\n",
    "\n",
    "                # # Log checkpoint info to wandb\n",
    "                # wandb.log({\n",
    "                #     \"checkpoint/last_saved_step\": global_step,\n",
    "                #     \"checkpoint/tokens_at_save\": tokens_processed\n",
    "                # }, step=global_step)\n",
    "\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Step {global_step}/{config.total_steps} [Train]\")\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_step_loss:.4f}\",\n",
    "                \"grad_norm\": f\"{np.mean(step_grad_norms):.4f}\",\n",
    "                \"lr\": f\"{current_lr:.6f}\",\n",
    "                \"tokens\": f\"{tokens_processed:,}\",\n",
    "                \"micro_batches\": f\"{config.gradient_accumulation_steps}\"\n",
    "            })\n",
    "\n",
    "            # Break if we've reached the total steps\n",
    "            if global_step >= config.total_steps:\n",
    "                print(f\"\\nReached maximum steps ({config.total_steps}), stopping training...\")\n",
    "                break\n",
    "\n",
    "            # Break out of epoch loop if we've reached max steps\n",
    "            if global_step >= config.total_steps:\n",
    "                break\n",
    "\n",
    "            # Validation\n",
    "            val_losses = []\n",
    "            # val_accs = []\n",
    "\n",
    "            # Simple validation without iterator complexity\n",
    "            # pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "\n",
    "            if(global_step % config.eval_steps == 0):\n",
    "\n",
    "              rng, step_key = jax.random.split(rng)\n",
    "              for batch in tqdm(range(config.eval_steps), desc='Validation running...'):\n",
    "                  try:\n",
    "                    batch = next(val_data_iterator)\n",
    "                    # batch = batch[0]  # Unpack from tuple\n",
    "                  except StopIteration:\n",
    "                    # val_data_iterator = prefetch_to_device(iter(val_loader), 2, None)\n",
    "                    val_data_iterator = iter(val_loader)\n",
    "                    batch = next(val_data_iterator)\n",
    "                    # batch = batch[0]  # Unpack from tuple\n",
    "\n",
    "\n",
    "                  batch = jnp.array(batch)\n",
    "                  batch = jax.device_put(batch, jax.devices()[0])  # Manual device placement\n",
    "                  loss, _ = eval_step(state, batch, step_key)\n",
    "                  val_losses.append(float(loss))\n",
    "                  # val_accs.append(float(acc))\n",
    "                  print(loss)\n",
    "                  break\n",
    "                  # Update progress bar\n",
    "                  pbar.set_postfix({\n",
    "                      \"loss val\": f\"{loss:.4f}\"\n",
    "                      # \"acc\": f\"{acc:.4f}\"\n",
    "                  })\n",
    "\n",
    "            # Calculate epoch metrics\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            avg_train_grad_norm = np.mean(train_grad_norms)\n",
    "            if len(val_losses) > 0:\n",
    "              avg_val_loss = np.mean(val_losses)\n",
    "            else:\n",
    "              avg_val_loss = -1\n",
    "            # avg_val_acc = np.mean(val_accs)\n",
    "\n",
    "            # Log epoch metrics to wandb\n",
    "            wandb.log({\n",
    "                \"epoch/train_loss\": avg_train_loss,\n",
    "                \"epoch/val_loss\": avg_val_loss,\n",
    "                # \"epoch/val_accuracy\": avg_val_acc,\n",
    "                \"epoch/train_grad_norm\": avg_train_grad_norm,\n",
    "                \"epoch/epoch\": epoch + 1\n",
    "            }, step=global_step)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Grad Norm: {avg_train_grad_norm:.4f}\")\n",
    "            # print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
    "            print(f\"  Tokens Processed: {tokens_processed:,}\")\n",
    "            print(f\"  Global Step: {global_step}\")\n",
    "\n",
    "            # Save checkpoint at end of each epoch\n",
    "            # save_checkpoint(state, global_step, checkpoint_dir)\n",
    "\n",
    "    # Save final checkpoint\n",
    "    # print(\"\\n Saving final checkpoint...\")\n",
    "    # save_checkpoint(state, global_step, checkpoint_dir)\n",
    "\n",
    "    # Generate some text\n",
    "    print(\"\\nGenerating text...\")\n",
    "    generated = generate_text(state, \"The future of artificial intelligence\", tokenizer, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    save_to_file(generated, global_step)\n",
    "\n",
    "    # Log final generation to wandb\n",
    "    wandb.log({\n",
    "        \"generation/sample_text\": generated,\n",
    "        \"generation/prompt\": \"The future of artificial intelligence\",\n",
    "        \"training/final_step\": global_step,\n",
    "        \"training/final_tokens\": tokens_processed\n",
    "    })\n",
    "\n",
    "    # Log final training summary\n",
    "    log_training_summary(state, config, total_params, tokens_processed)\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f407e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "22f407e8",
    "outputId": "7733b76e-21a0-4a7d-cb32-01e669610f8d"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tawnWkOgHONx",
   "metadata": {
    "id": "tawnWkOgHONx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trainLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
