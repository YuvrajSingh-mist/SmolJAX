{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3d12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
      "/opt/miniconda3/envs/trainLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.training import checkpoints\n",
    "from flax import struct\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Any\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8385dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Information:\n",
      "JAX version: 0.6.2\n",
      "Platform: cpu\n",
      "Device count: 1\n",
      "❌ Error checking TPU: Backend 'tpu' failed to initialize: INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/miniconda3/envs/trainLLM/bin/../lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file). Available backends are ['cpu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/gs0prjr16mb6pdlzc09xy07c0000gn/T/ipykernel_31468/3071427205.py:33: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(f\"Platform: {jax.lib.xla_bridge.get_backend().platform}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test computation result: 55\n",
      "Computation device: TFRT_CPU_0\n",
      "jax.devices(): [CpuDevice(id=0)]\n",
      "jax.device_count(): 1\n",
      "jax.local_device_count(): 1\n",
      "jax.process_count(): 1\n",
      "jax.process_index(): 0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# Check TPU availability and setup\n",
    "def check_tpu_setup():\n",
    "    \"\"\"Check if TPU is available and properly configured.\"\"\"\n",
    "    try:\n",
    "        # Check if we're running on TPU\n",
    "        if jax.devices('tpu'):\n",
    "            print(\"✅ TPU detected!\")\n",
    "            devices = jax.devices('tpu')\n",
    "            print(f\"   TPU devices: {len(devices)}\")\n",
    "            for i, device in enumerate(devices):\n",
    "                print(f\"   Device {i}: {device}\")\n",
    "\n",
    "            # Print TPU-specific info\n",
    "            print(f\"   JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "            print(f\"   Total TPU cores: {jax.device_count()}\")\n",
    "            print(f\"   Local devices: {jax.local_device_count()}\")\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ No TPU devices found\")\n",
    "            print(f\"   Available devices: {jax.devices()}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking TPU: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check current setup\n",
    "print(\"Device Information:\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Platform: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "\n",
    "# Check TPU specifically\n",
    "has_tpu = check_tpu_setup()\n",
    "\n",
    "# Test a simple computation\n",
    "test_array = jnp.array([1, 2, 3, 4, 5])\n",
    "result = jnp.sum(test_array ** 2)\n",
    "print(f\"\\nTest computation result: {result}\")\n",
    "print(f\"Computation device: {result.device}\")\n",
    "\n",
    "print(\"jax.devices():\", jax.devices())\n",
    "print(\"jax.device_count():\", jax.device_count())\n",
    "print(\"jax.local_device_count():\", jax.local_device_count())\n",
    "print(\"jax.process_count():\", jax.process_count())\n",
    "print(\"jax.process_index():\", jax.process_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9b95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['FLAX_USE_LEGACY_CHECKPOINTS'] = '1'  # Force legacy checkpoints to avoid Orbax issues\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7726e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=\"\")\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03b04945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for model parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size_english: int = 25000\n",
    "    vocab_size_hindi: int = 25000\n",
    "    max_seq_len: int = 64\n",
    "    d_model: int = 512\n",
    "    num_layers: int = 6\n",
    "    num_heads: int = 8\n",
    "    d_ff: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    lr: float = 6e-4\n",
    "    min_lr: float = 0.1 * lr\n",
    "    warmup_steps: int = 700\n",
    "    total_steps: int = 20000\n",
    "    batch_size: int = 256\n",
    "    required_bsz_tokens: int = 524288\n",
    "    gradient_accumulation_steps: int = int(required_bsz_tokens // (batch_size * max_seq_len))\n",
    "    mixed_precision: bool = True\n",
    "    num_epochs: int = 1\n",
    "    eval_steps: int = 200\n",
    "\n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "296c002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set JAX matmul precision to bfloat16 (mixed precision enabled)\n"
     ]
    }
   ],
   "source": [
    "from jax import config as jax_config\n",
    "\n",
    "# Set matmul precision based on mixed_precision setting\n",
    "if config.mixed_precision:\n",
    "    jax_config.update(\"jax_default_matmul_precision\", \"bfloat16\")\n",
    "    print(\"Set JAX matmul precision to bfloat16 (mixed precision enabled)\")\n",
    "else:\n",
    "    jax_config.update(\"jax_default_matmul_precision\", \"float32\")\n",
    "    print(\"Set JAX matmul precision to float32 (mixed precision disabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0fea612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: <class 'jax.numpy.bfloat16'> (mixed_precision=True)\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get the appropriate dtype based on mixed_precision setting\n",
    "def get_dtype():\n",
    "    \"\"\"Return bfloat16 if mixed_precision is True, else float32.\"\"\"\n",
    "    return jnp.bfloat16 if config.mixed_precision else jnp.float32\n",
    "\n",
    "print(f\"Using dtype: {get_dtype()} (mixed_precision={config.mixed_precision})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "716a9ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trainLLM/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15f4b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    grad_accum: Any = None\n",
    "    accum_step: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38fa538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/yuvrajsingh9886/.cache/huggingface/datasets/roneneldan___tiny_stories/default/0.0.0/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Tue Sep 23 04:41:01 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /Users/yuvrajsingh9886/.cache/huggingface/datasets/roneneldan___tiny_stories/default/0.0.0/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Tue Sep 23 04:41:01 2025).\n",
      "Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/yuvrajsingh9886/.cache/huggingface/datasets/roneneldan___tiny_stories/default/0.0.0/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Tue Sep 23 04:41:01 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /Users/yuvrajsingh9886/.cache/huggingface/datasets/roneneldan___tiny_stories/default/0.0.0/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Tue Sep 23 04:41:01 2025).\n"
     ]
    }
   ],
   "source": [
    "#Loading TinyStories dataset from Huggingface\n",
    "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", token='')\n",
    "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3ae033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "\n",
    "        # Proper initialization for attention layers\n",
    "        self.d_Q = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_K = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_V = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_O = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        B, T, C = x.shape\n",
    "        query = self.d_Q(x)\n",
    "        key = self.d_K(x)\n",
    "        value = self.d_V(x)\n",
    "\n",
    "        # Proper attention scaling using head_size\n",
    "        weights = jnp.matmul(query, key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
    "\n",
    "        # # Better causal mask using -inf\n",
    "        # mask = jnp.tril(jnp.ones((T, T)))\n",
    "        # weights = jnp.where(mask == 0, -jnp.inf, weights)\n",
    "\n",
    "        weights = nn.softmax(weights, axis=-1)\n",
    "        # weights = self.dropout(weights, deterministic=not training)  # Apply dropout to attention weights\n",
    "\n",
    "        out = jnp.matmul(weights, value)\n",
    "        out = self.d_O(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eadac2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullMHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [SelfAttention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "\n",
    "        # Proper initialization for output projection\n",
    "        self.linear = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "129d2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "\n",
    "        # Proper initialization for attention layers\n",
    "        self.d_Q = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_K = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_V = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_O = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        B, T, C = x.shape\n",
    "        query = self.d_Q(x)\n",
    "        key = self.d_K(x)\n",
    "        value = self.d_V(x)\n",
    "\n",
    "        # Proper attention scaling using head_size\n",
    "        weights = jnp.matmul(query, key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
    "\n",
    "        # Better causal mask using -inf\n",
    "        mask = jnp.tril(jnp.ones((T, T)))\n",
    "        weights = jnp.where(mask == 0, -jnp.inf, weights)\n",
    "\n",
    "        weights = nn.softmax(weights, axis=-1)\n",
    "        # weights = self.dropout(weights, deterministic=not training)  # Apply dropout to attention weights\n",
    "\n",
    "        out = jnp.matmul(weights, value)\n",
    "        out = self.d_O(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3b436b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [MaskedSelfAttention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "\n",
    "        # Proper initialization for output projection\n",
    "        self.linear = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        out = jnp.concatenate([head(x, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01b1bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.head_size = self.d_model // self.num_heads\n",
    "\n",
    "        # Proper initialization for attention layers\n",
    "        self.d_Q = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_K = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_V = nn.Dense(\n",
    "            features=self.head_size,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.d_O = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            use_bias=False,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, q, k, v, training=True):\n",
    "        B, T, C = q.shape\n",
    "        query = self.d_Q(q)\n",
    "        key = self.d_K(k)\n",
    "        value = self.d_V(v)\n",
    "\n",
    "        # Proper attention scaling using head_size\n",
    "        weights = jnp.matmul(query, key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
    "\n",
    "       \n",
    "\n",
    "        weights = nn.softmax(weights, axis=-1)\n",
    "        # weights = self.dropout(weights, deterministic=not training)  # Apply dropout to attention weights\n",
    "\n",
    "        out = jnp.matmul(weights, value)\n",
    "        out = self.d_O(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f0ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMHA(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [CrossAttention(self.d_model, self.num_heads, self.dropout_rate) for _ in range(self.num_heads)]\n",
    "\n",
    "        # Proper initialization for output projection\n",
    "        self.linear = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, q, k, v, training=True):\n",
    "        out = jnp.concatenate([head(q, k, v, training) for head in self.heads], axis=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out, deterministic=not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be2cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        # Proper initialization for MLP layers\n",
    "        self.fc1 = nn.Dense(\n",
    "            features=self.d_ff,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.fc2 = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x, deterministic=not training)  # Remove duplicate GELU\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a78d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.masked_attention = MaskedMHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.cross_attention = CrossMHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm(dtype=get_dtype())\n",
    "        self.ln2 = nn.LayerNorm(dtype=get_dtype())\n",
    "        self.ln3 = nn.LayerNorm(dtype=get_dtype())\n",
    "        \n",
    "    def __call__(self, x, k, v, training=True):\n",
    "\n",
    "        out = self.ln1(x + self.masked_attention(x, training))\n",
    "        cross_attn = self.ln2(out + self.cross_attention(out, k, v, training))\n",
    "        mlp_out = self.ln3(cross_attn + self.mlp(cross_attn, training))\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de0fe21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    d_model: int = config.d_model\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = FullMHA(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.mlp = MLP(self.d_model, self.d_ff, self.dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm(dtype=get_dtype())\n",
    "        self.ln2 = nn.LayerNorm(dtype=get_dtype())\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        attn =  self.ln1(self.attention(x, training))\n",
    "        # x = x + attn * ((2 * config.num_layers ** -0.5))\n",
    "        mlp_out = self.ln2(self.mlp(attn, training))\n",
    "        # x = x + mlp_out * (2 * (config.num_layers ** -0.5))\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9929f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    vocab_size__english: int = config.vocab_size_english\n",
    "    vocab_size__hindi: int = config.vocab_size_hindi\n",
    "    max_seq_len: int = config.max_seq_len\n",
    "    d_model: int = config.d_model\n",
    "    num_layers: int = config.num_layers\n",
    "    num_heads: int = config.num_heads\n",
    "    d_ff: int = config.d_ff\n",
    "    dropout_rate: float = config.dropout_rate\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_eng = nn.Embed(\n",
    "            num_embeddings=self.vocab_size__english,\n",
    "            features=self.d_model,\n",
    "            embedding_init=nn.initializers.normal(stddev=0.02),\n",
    "            dtype=get_dtype()\n",
    "        )\n",
    "        self.token_embedding_hindi = nn.Embed(\n",
    "            num_embeddings=self.vocab_size__hindi,\n",
    "            features=self.d_model,\n",
    "            embedding_init=nn.initializers.normal(stddev=0.02),\n",
    "            dtype=get_dtype()\n",
    "        )\n",
    "        self.positional_embedding_english = self.param(\n",
    "            \"positional_embeddings_english\",  # name\n",
    "            lambda key: jax.random.normal(key, (1, self.max_seq_len, self.d_model), dtype=get_dtype()) * 0.01\n",
    "        )\n",
    "        self.positional_embedding_hindi = self.param(\n",
    "            \"positional_embeddings_hindi\",  # name\n",
    "            lambda key: jax.random.normal(key, (1, self.max_seq_len, self.d_model), dtype=get_dtype()) * 0.01\n",
    "        )\n",
    "        self.encoder_layers = [EncoderBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(self.num_layers)]\n",
    "        self.decoder_layers = [DecoderBlock(self.d_model, self.num_heads, self.d_ff, self.dropout_rate) for _ in range(self.num_layers)]\n",
    "        # self.ln_f = nn.LayerNorm(dtype=get_dtype())\n",
    "        self.head = nn.Dense(\n",
    "            features=self.vocab_size__hindi,\n",
    "            dtype=get_dtype(),\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, enc_input, dec_input, training=True):\n",
    "        B, T_enc = enc_input.shape\n",
    "        B, T_dec = dec_input.shape\n",
    "\n",
    "        # Encoder\n",
    "        enc_x = self.token_embedding_eng(enc_input) + self.positional_embedding_english[:, :T_enc, :]\n",
    "        enc_x = self.dropout(enc_x, deterministic=not training)\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_x = layer(enc_x, training)\n",
    "\n",
    "        # Decoder\n",
    "        dec_x = self.token_embedding_hindi(dec_input) + self.positional_embedding_hindi[:, :T_dec, :]\n",
    "        dec_x = self.dropout(dec_x, deterministic=not training)\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_x = layer(dec_x, enc_x, enc_x, training)\n",
    "\n",
    "        # dec_x = self.ln_f(dec_x)\n",
    "        logits = self.head(dec_x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5940eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved to model_summary.txt\n",
      "Total Parameters: 120,350,120\n",
      "Model size: ~229.5 MB (bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to inspect the model summary like torchsummary\n",
    "\n",
    "from flax.linen import tabulate\n",
    "import jax\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer()\n",
    "key = jax.random.PRNGKey(0)\n",
    "x1 = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # Dummy input for tabulation\n",
    "x2 = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # Dummy input for tabulation\n",
    "\n",
    "# Tabulate the model structure\n",
    "tabulate_fn = tabulate(model, key, console_kwargs={'width': 120})\n",
    "\n",
    "# Count total parameters\n",
    "params = model.init(key, x1, x2)['params']\n",
    "total_params = sum(jax.tree_util.tree_leaves(jax.tree.map(lambda arr: arr.size, params)))\n",
    "\n",
    "# Get raw summary and clean ANSI codes\n",
    "raw_summary = tabulate_fn(x1, x2, training=True)\n",
    "# Remove ANSI color codes for clean logging\n",
    "clean_summary = re.sub(r'\\x1b\\[[0-9;]*m', '', raw_summary)\n",
    "\n",
    "# Save to log file with clean formatting\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"TRANSFORMER MODEL ARCHITECTURE SUMMARY\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"Model Configuration:\\n\")\n",
    "    f.write(f\"  - Vocabulary Size English: {config.vocab_size_english:,}\\n\")\n",
    "    f.write(f\"  - Vocabulary Size Hindi: {config.vocab_size_hindi:,}\\n\")\n",
    "    f.write(f\"  - Max Sequence Length: {config.max_seq_len}\\n\")\n",
    "    f.write(f\"  - Model Dimension: {config.d_model}\\n\")\n",
    "    f.write(f\"  - Number of Layers: {config.num_layers}\\n\")\n",
    "    f.write(f\"  - Number of Heads: {config.num_heads}\\n\")\n",
    "    f.write(f\"  - Feed Forward Dimension: {config.d_ff}\\n\")\n",
    "    f.write(f\"  - Dropout Rate: {config.dropout_rate}\\n\\n\")\n",
    "    f.write(\"Detailed Layer Information:\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(clean_summary)\n",
    "\n",
    "print(f\"Model summary saved to model_summary.txt\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 2 / (1024**2):.1f} MB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afe17435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_rate_schedule():\n",
    "    \"\"\"Create a learning rate schedule with warmup and cosine decay.\"\"\"\n",
    "    # Use values from config\n",
    "    max_lr = config.lr  # 6e-4\n",
    "    min_lr = config.min_lr\n",
    "    warmup_steps = config.warmup_steps  # 700 (or can override to 715)\n",
    "    max_steps = config.total_steps  # 20000 (or can override to 19073)\n",
    "    \n",
    "    def get_lr(it):\n",
    "\n",
    "        return config.num_layers ** -0.5 * min(it ** -0.5, it * warmup_steps ** -1.5)\n",
    "\n",
    "    return get_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0352ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    labels = labels[:, 1:]\n",
    "    logits = logits[:, :-1, :]  # Shift logits to align with labels\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    pad_mask = (labels != tokenizer.pad_token_id)\n",
    "    loss = jnp.where(pad_mask, loss, 0.0)\n",
    "    return loss.sum () / pad_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "    model = Transformer()\n",
    "\n",
    "    # Initialize parameters\n",
    "    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)\n",
    "    dummy_input2 =  jnp.ones((1, config.max_seq_len), dtype=jnp.int32)  # For encoder and decoder inputs\n",
    "    params = model.init(rng, (dummy_input, dummy_input2))['params']\n",
    "\n",
    "    # Create learning rate schedule\n",
    "    lr_schedule = create_learning_rate_schedule()\n",
    "\n",
    "    # Create optimizer with stronger gradient clipping and better settings\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),  # Much stronger clipping (was 1.0)\n",
    "        optax.adam(\n",
    "            learning_rate=lr_schedule,\n",
    "            b1=0.9,\n",
    "            b2=0.98,\n",
    "            # weight_decay=0.01,  # Reduced weight decay (was 0.1)\n",
    "            eps=1e-9  # Added epsilon for numerical stability\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, step):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, batch, training=True, rngs={'dropout': step})\n",
    "        loss = compute_ce_loss(logits, batch)\n",
    "        return loss, logits\n",
    "\n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "\n",
    "    # Compute gradient norm for logging\n",
    "    grad_norm = jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in jax.tree_util.tree_leaves(grads)]))\n",
    "\n",
    "    # Update the parameters\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compiled evaluation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch, step):\n",
    "    \"\"\"Single evaluation step.\"\"\"\n",
    "    logits = state.apply_fn({'params': state.params}, batch, training=False, rngs={'dropout': step})\n",
    "    loss = compute_ce_loss(logits, batch)\n",
    "\n",
    "    return loss, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
